[
  {
    "objectID": "cdi/index.html",
    "href": "cdi/index.html",
    "title": "Accessing the USGS National Map and making 3D landscapes with terrainr",
    "section": "",
    "text": "This is an R Markdown document containing code for the workshop “Accessing the USGS National Map and making 3D maps with terrainr”, held virtually on 2021-05-28. If you want to follow along with the workshop as we go, click this link to download the R Markdown notebook.\nIf you’re not familiar with R Markdown, this document lets us write both plain text and code in a single document. Document sections inside three ` marks are called “code chunks”:\nplot(Orange)\nA single line in a code chunk can be run by pressing Control+Enter with your cursor on the line. The entire chunk can be run by pressing Control+Shift+Enter with your cursor inside the chunk."
  },
  {
    "objectID": "cdi/index.html#example",
    "href": "cdi/index.html#example",
    "title": "Accessing the USGS National Map and making 3D landscapes with terrainr",
    "section": "Example",
    "text": "Example\nNow that we’ve seen a basic workflow using terrainr to pull layers from the National Map and plot with ggplot2, let’s have some fun with it! Starting with the latitude and longitude coordinates for a new site, we’ll create an sf object and add a spatial buffer around it like before.\n\n# yosemite NP\nsite &lt;- data.frame(lat = 37.7456, lng = -119.5521)\n\n# convert to sf object and assign coordinate reference system \nsite_sf &lt;- st_as_sf(site, coords = c(\"lng\",\"lat\"), crs = 4326)\n\n# buffer around point\nsite_bbox &lt;- set_bbox_side_length(site_sf, 10, \"km\")\n\nNow, to pull some data using terrainr. In the previous example we looked at elevation and orthoimagery. Other supported services are: nhd, govunits, contours, geonames, NHDPlus_HR, structures, transportation, and wbd. More info here: https://docs.ropensci.org/terrainr/#available-datasets\nLet’s try elevation and contours this time:\n\nwith_progress(\n  site_tiles &lt;- get_tiles(site_bbox, \n                            output_prefix = \"yosemite_np_5m\",\n                            services = c(\"elevation\", \"contours\"),\n                            resolution = 5)\n)\n\nsite_tiles\n\n$elevation\n[1] \"yosemite_np_5m_3DEPElevation_1_1.tif\"\n\n$contours\n[1] \"yosemite_np_5m_contours_1_1.tif\"\n\n\nNow we can convert the elevation and contour layers to plot with ggplot2. The contours layer is an RGBA multi-band raster layer, similar to the orthoimagery.\n\nelevation &lt;- raster(site_tiles$elevation)\ncontours &lt;- stack(site_tiles$contours)\n\n# look at structure of contours raster - has 4 layers\ncontours\n\nclass      : RasterStack \ndimensions : 2000, 1999, 3998000, 4  (nrow, ncol, ncell, nlayers)\nresolution : 0.00005689511, 0.00005689511  (x, y)\nextent     : -119.6089, -119.4952, 37.68869, 37.80248  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nnames      :     lyr.1,     lyr.2,     lyr.3,     lyr.4 \nmin values : 0.6000000, 0.4117647, 0.2235294, 0.0000000 \nmax values :         1,         1,         1,         1 \n\n\nRGBA has 4 channels: red, green, blue, and alpha. The 4th channel, alpha, is like opacity. In this case, alpha is 0 in cells without contour lines and 1 where contours exist.\n\nggplot() + \n  geom_spatial_rgb(data = contours,\n                   mapping = aes(x, y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326) + \n  theme_void()\n\n\n\n\n\n\n\n\nContours reflect elevation, so let’s make it so! The next chunk stacks elevation and contours together and coverts to a dataframe to plot using geom_raster with ggplot2. Then, using the alpha channel, the dataframe is filtered to just the contour lines, and elevation values are used in the color scale.\n\ncontour_stack &lt;- stack(elevation, contours)\ncontour_df &lt;- as.data.frame(contour_stack, xy = TRUE)\n\nnames(contour_df) &lt;- c(\"x\", \"y\", \"z\", \"r\", \"g\", \"b\", \"a\") # longitude, latitude, elevation, red, green, blue, alpha\n\ncontour_lines &lt;- contour_df[contour_df$a != 0, ]\n\n## plot it!\nggplot(data = contour_lines)+\n  geom_raster(mapping = aes(x, y, fill = z))+\n  theme_void()+\n  theme(plot.background = element_rect(fill=\"black\"),\n        plot.title = element_text(color=\"white\"),\n        legend.position = \"none\")+\n  coord_sf(crs = 4326)+\n  scale_fill_scico(palette=\"imola\", direction = 1)+\n  ggtitle(paste(\"Half dome\", site$lat, site$lng))\n\n\n\n\n\n\n\n\nThis chart uses the scico package for color scale. Scico (https://github.com/thomasp85/scico) offers a selection of color palettes developed for scientific applications. They are perceptually uniform to represent data fairly, and universally readable by both color-vision deficient and color-blind people.\n\nscico_palette_show() # to see all palette options\n\n\n\n\n\n\n\n\nFor the purposes of the workshop, we’re avoiding large downloads. But if we were to increase the resolution even more, say to 1m, terrainr might break up the focal region into multiple tiles:\n\n## warning: running this code chunk will take over 5 minutes to complete\nwith_progress(\n  site_tiles_1 &lt;- get_tiles(site_bbox, \n                            output_prefix = \"yosemite_np_10m\",\n                            services = c(\"elevation\", \"contours\"),\n                            resolution = 1)\n)\n\nsite_tiles_1\n\nIn which case they can be merged together using merge_rasters\n\nmerge_el &lt;- terrainr::merge_rasters(site_tiles_1$elevation, \n                          tempfile(fileext = \".tif\"))\n\nmerge_co &lt;- terrainr::merge_rasters(site_tiles_1$contours, \n                          tempfile(fileext = \".tif\"))"
  },
  {
    "objectID": "cdi/index.html#on-your-own",
    "href": "cdi/index.html#on-your-own",
    "title": "Accessing the USGS National Map and making 3D landscapes with terrainr",
    "section": "On your own",
    "text": "On your own\nUse the dataframe below to pull new starting coordinates and recreate the map in a new location, with new layer, or maybe a new color palette. Share what you made in the chat!\n\n# dataframe of cool places in the US\nplaces &lt;- structure(list(name = c(\"half dome\", \"moki dugway\", \"badwater basin\", \n\"mount whitney\", \"monument valley navajo tribal park\", \"devil's lake\", \n\"cadillac mountain\", \"mcafee knob\", \"sleeping bear dunes \", \"grand canyon\", \n\"miguel's pizza at red river gorge\", \"between cumberland and blue ridge mountains\", \n\"missouri river floodplain and bluff\", \"shawangunk mountains\", \n\"lake winnipesaukee, new hampshire\", \"mount washington, new hampshire\", \n\"gravity hill, pa\", \"orcas island\", \"grand canyon of yellowstone\", \n\"mauna loa\", \"tetons\", \"rocky mountain biological station\"), \n    lat = c(37.746036, 37.302306, 36.250278, 36.578581, 36.983333, \n    43.420033, 44.35127, 37.392487, 44.878727, 36.2388, 37.783004, \n    37.1177098, 38.568762, 41.72469, 43.606096, 44.270504, 40.153283, \n    48.654167, 44.895556, 19.479444, 43.75, 38.958611), lng = c(-119.53294, \n    -109.968944, -116.825833, -118.291995, -110.100411, -89.737405, \n    -68.22649, -80.036298, -86.066458, -112.350427, -83.682861, \n    -81.132816, -90.883408, -74.204946, -71.338624, -71.303115, \n    -76.839954, -122.938333, -110.389444, -155.602778, -110.833333, \n    -106.987778)), class = \"data.frame\", row.names = c(NA, -22L\n))\n\n# randomly pick one row from the dataframe above\nsite &lt;- places[sample(nrow(places), 1), ]\n\n# Or set your own by changing the values below and uncommenting\n# site &lt;- data.frame(ï..name = \"skywalk @ grand canyon\", lat = 36.011827, lng = -113.810931) \n\n# convert to sf object and assign coordinate reference system \nsite_sf &lt;- st_as_sf(site, coords = c(\"lng\",\"lat\"), crs = 4326)\n\n# buffer around point\nsite_bbox &lt;- set_bbox_side_length(site_sf, 10, \"km\")\n\n# keep going! Using code from previous steps, make the rest of the map.\n\n# Create site name for use as filenames:\nsite_fn &lt;- gsub(\" \", \"_\", gsub(\"[[:punct:]]\", \"\", site$name))\n\n# Download the appropriate data for your site from The National Map\nwith_progress(\n  site_tiles &lt;- get_tiles(site_bbox, \n                          output_prefix = sprintf(\"%s_5m\", site_fn),\n                          services = c(\"elevation\", \"contours\"),\n                          resolution = 5)\n)\n\n# COnvert to a raster stack\nelevation &lt;- raster(site_tiles$elevation)\ncontours &lt;- stack(site_tiles$contours)\ncontour_stack &lt;- stack(elevation, contours)\ncontour_df &lt;- as.data.frame(contour_stack, xy = TRUE)\nnames(contour_df) &lt;- c(\"x\", \"y\", \"z\", \"r\", \"g\", \"b\", \"a\") # longitude, latitude, elevation, red, green, blue, alpha\n\n# Remove zero elevations\ncontour_lines &lt;- contour_df[contour_df$a != 0, ]\n\n## plot it!\nggplot(data = contour_lines)+\n  geom_raster(mapping = aes(x, y, fill = z))+\n  theme_void()+\n  theme(plot.background = element_rect(fill=\"black\"),\n        plot.title = element_text(color=\"white\"),\n        legend.position = \"none\")+\n  coord_sf(crs = 4326)+\n  # Change the color palette used by editing the `palette` argument below!!\n  scale_fill_scico(palette=\"imola\", direction = 1)+\n  ggtitle(paste(site$name, site$lat, site$lng))\n\n\n\n\n\n\n\n# If you want to save a higher resolution version (it makes the lines a little sharper)\n# Run this after printing your plot:\n# ggsave(sprintf(\"whimsical_topomap_terrainr_%s.png\", site_fn), width=5, height=8)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Cebulski",
    "section": "",
    "text": "PhD candidate studying snow accumulation in mountain forests at the University of Saskatchewan’s Centre for Hydrology in Canmore, AB.\n\n\n\n\n\n\n\nCebulski, A and Desloges, J. R. (2024). The Influence of Fluvial and Glacial Watershed Dynamics on Holocene Sediment Accumulation in Cariboo Lake, Columbia Mountains, British Columbia, Canada. Canadian Journal of Earth Sciences. LINK\nMcNicol, G, Hood, E, Butman, DE, Tank, SE, Giesbrecht, IJW, Floyd, W, D’Amore, D, Fellman, JB, Cebulski, A, Lally, A, McSorley, H, Arriola, SG. (2023). Small, coastal temperate rainforest watersheds dominate dissolved organic carbon transport to the northeast Pacific Ocean. Geophysical Research Letters. LINK\n\n\n\nCHRL-weather-data | A website to visualize hydrometeorological data for the Vancouver Island University - Coastal Hydrology Research Lab network of weather stations | 2020\nCCRN-weather-data | A website to visualize hydrometeorological data for the University of Saskatchewan - Cold Regions Hydrological Observatory network of weather stations | 2021\n\n\n\nweatherdash | Methods for weather dashboards | 2022\nwxlogR | For working with hydrometeorological station data logger readouts | 2021\npsychRomet | Collection of Equations for Psychrometric Calculations | 2021"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Alex Cebulski",
    "section": "",
    "text": "PhD candidate studying snow accumulation in mountain forests at the University of Saskatchewan’s Centre for Hydrology in Canmore, AB."
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Alex Cebulski",
    "section": "",
    "text": "Cebulski, A and Desloges, J. R. (2024). The Influence of Fluvial and Glacial Watershed Dynamics on Holocene Sediment Accumulation in Cariboo Lake, Columbia Mountains, British Columbia, Canada. Canadian Journal of Earth Sciences. LINK\nMcNicol, G, Hood, E, Butman, DE, Tank, SE, Giesbrecht, IJW, Floyd, W, D’Amore, D, Fellman, JB, Cebulski, A, Lally, A, McSorley, H, Arriola, SG. (2023). Small, coastal temperate rainforest watersheds dominate dissolved organic carbon transport to the northeast Pacific Ocean. Geophysical Research Letters. LINK\n\n\n\nCHRL-weather-data | A website to visualize hydrometeorological data for the Vancouver Island University - Coastal Hydrology Research Lab network of weather stations | 2020\nCCRN-weather-data | A website to visualize hydrometeorological data for the University of Saskatchewan - Cold Regions Hydrological Observatory network of weather stations | 2021\n\n\n\nweatherdash | Methods for weather dashboards | 2022\nwxlogR | For working with hydrometeorological station data logger readouts | 2021\npsychRomet | Collection of Equations for Psychrometric Calculations | 2021"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Papers",
    "section": "",
    "text": "Click [LINK] to download each paper.\nCebulski, A and Desloges, J. R. (2024). The Influence of Fluvial and Glacial Watershed Dynamics on Holocene Sediment Accumulation in Cariboo Lake, Columbia Mountains, British Columbia, Canada. Canadian Journal of Earth Sciences. LINK\nMcNicol, G, Hood, E, Butman, DE, Tank, SE, Giesbrecht, IJW, Floyd, W, D’Amore, D, Fellman, JB, Cebulski, A, Lally, A, McSorley, H, Arriola, SG. (2023). Small, coastal temperate rainforest watersheds dominate dissolved organic carbon transport to the northeast Pacific Ocean. Geophysical Research Letters. LINK"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Alex Cebulski",
    "section": "",
    "text": "Central coast BC weather station service\n\n\n\n\n\n\nweather station\n\n\nservice\n\n\nCHRL\n\n\n\nVisit to the coastal hydrology research lab and bc ministry of environment central coast hydrometric stations to conduct snow courses and service the instruments.\n\n\n\n\n\nMar 25, 2021\n\n\nAlex Cebulski\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "junkdrawer/a-simple-script/index.html#speed-versus-capacity",
    "href": "junkdrawer/a-simple-script/index.html#speed-versus-capacity",
    "title": "Cool graphs about elevators",
    "section": "Speed versus capacity",
    "text": "Speed versus capacity"
  },
  {
    "objectID": "junkdrawer/a-simple-script/index.html#where-in-the-world-did-all-my-elevators-go",
    "href": "junkdrawer/a-simple-script/index.html#where-in-the-world-did-all-my-elevators-go",
    "title": "Cool graphs about elevators",
    "section": "Where in the world did all my elevators go",
    "text": "Where in the world did all my elevators go"
  },
  {
    "objectID": "junkdrawer/parameterized-reports/manhattan/index.html#speed-versus-capacity",
    "href": "junkdrawer/parameterized-reports/manhattan/index.html#speed-versus-capacity",
    "title": "Cool graphs about elevators",
    "section": "Speed versus capacity",
    "text": "Speed versus capacity"
  },
  {
    "objectID": "junkdrawer/parameterized-reports/manhattan/index.html#where-in-the-world-did-all-my-elevators-go",
    "href": "junkdrawer/parameterized-reports/manhattan/index.html#where-in-the-world-did-all-my-elevators-go",
    "title": "Cool graphs about elevators",
    "section": "Where in the world did all my elevators go",
    "text": "Where in the world did all my elevators go"
  },
  {
    "objectID": "junkdrawer/parameterized-reports/index.html#speed-versus-capacity",
    "href": "junkdrawer/parameterized-reports/index.html#speed-versus-capacity",
    "title": "Cool graphs about elevators",
    "section": "Speed versus capacity",
    "text": "Speed versus capacity"
  },
  {
    "objectID": "junkdrawer/parameterized-reports/index.html#where-in-the-world-did-all-my-elevators-go",
    "href": "junkdrawer/parameterized-reports/index.html#where-in-the-world-did-all-my-elevators-go",
    "title": "Cool graphs about elevators",
    "section": "Where in the world did all my elevators go",
    "text": "Where in the world did all my elevators go"
  },
  {
    "objectID": "junkdrawer/riemann/index.html",
    "href": "junkdrawer/riemann/index.html",
    "title": "Multi-scale model assessment with spatialsample",
    "section": "",
    "text": "Modeling spatially structured data is complicated. In addition to the usual difficulty of statistical modeling, models of spatially structured data may have spatial structure in their errors, with different regions being more or less well-described by a given model. This also means that accuracy metrics for these models might change depending on what spatial scale is being assessed. Only investigating model accuracy at larger aggregation scales, such as when accuracy is only assessed for the entire study area as a whole, might “smooth out” these local differences and present an inaccurate picture of model performance.\nFor this reason, a number of researchers (most notably, Riemann et al. (2010)1) have suggested assessing models at multiple scales of spatial aggregation to ensure cross-scale differences in model accuracy are identified and reported. This post walks through how to do that using the new spatialsample package.\nBecause Riemann et al. were working with data from the US Forest Inventory and Analysis (FIA) program, we’re going to do the same. However, because our main goal is to show how spatialsample can support this type of analysis, we won’t spend a ton of time worrying about any of the quirks of FIA data2 or on feature engineering. Instead, we’re going to use a simple linear model to see if we can predict how much aboveground biomass (“AGB”; all the non-root woody bits) there is in a forest based on how many trees there are. We’ll use all the FIA field data from New York State, USA.\nBecause we’re mostly interested in assessing our models, I’m going to mostly ignore how exactly I downloaded and wrangled the FIA data for this post. If you’re curious, I’ve hidden the code below:\nCodelibrary(dplyr)\n\n# Download the FIA database for New York over the internet,\n# and unzip it into our local directory\n#\n# This updates annually, which means that this post likely won't\n# generate the exact same results after 2022\nhttr::GET(\n  \"https://apps.fs.usda.gov/fia/datamart/Databases/SQLite_FIADB_NY.zip\",\n  httr::write_disk(\"SQLite_FIADB_NY.zip\", TRUE)\n)\n\nunzip(\"SQLite_FIADB_NY.zip\")\n\n# We're going to work with the database through dplyr's database connections\n#\n# But first, we need to create a DBI connection to the database and\n# load out tables:\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = \"FIADB_NY.db\")\ntrees &lt;- tbl(con, \"TREE\")\n\nplots &lt;- tbl(con, \"PLOT\")\n\n# The FIA database has every measurement ever collected by the program;\n# we'll filter to only the most recent survey for each of the plots.\n#\n# Plots are measured on a rolling 7 year basis, so we'll also cut out any\n# plots which might not be remeasured anymore with a call to filter()\nplots &lt;- plots |&gt; \n  group_by(PLOT) |&gt; \n  filter(INVYR == max(INVYR, na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  filter(INVYR &gt; 2009) |&gt; \n  collect()\n\ncopy_to(con, plots, \"newest_plots\", TRUE)\nnewest_plots &lt;- tbl(con, \"newest_plots\")\n\n# Now we'll use a filtering join to select only trees measured in the most\n# recent sample at each plot\n#\n# We'll also count how many trees were at each plot,\n# sum up their AGB, \n# and save out a few other useful columns like latitude and longitude\nplot_measurements &lt;- trees |&gt; \n  right_join(newest_plots, by = c(\"INVYR\", \"PLOT\")) |&gt; \n  group_by(PLOT) |&gt; \n  summarise(\n    yr = mean(INVYR, na.rm = TRUE),\n    plot = mean(PLOT, na.rm = TRUE),\n    lat = mean(LAT, na.rm = TRUE),\n    long = mean(LON, na.rm = TRUE),\n    n_trees = n(),\n    agb = sum(DRYBIO_AG, na.rm = TRUE)\n  ) |&gt; \n  collect() |&gt; \n  mutate(\n    # Because of how we joined, `n_trees` is always at least 1 -- \n    # even if there are 0 trees\n    n_trees = ifelse(is.na(agb) & n_trees == 1, 0, n_trees),\n    agb = ifelse(is.na(agb), 0, agb)\n  )\n\nDBI::dbDisconnect(con)\n\nreadr::write_csv(plot_measurements, \"plot_measurements.csv\")\nWith that pre-processing done, it’s time for us to load our data and turn it into an sf object. We’re going to reproject our data to use a coordinate reference system that the US government tends to use for national data products, like the FIA:\nlibrary(sf)\n\ninvisible(sf_proj_network(TRUE))\n\nplot_measurements &lt;- readr::read_csv(\"https://www.mm218.dev/junkdrawer/riemann/plot_measurements.csv\") |&gt; \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) |&gt; \n  st_transform(5070)\nAnd this is what we’re going to go ahead and resample. We want to assess our model’s performance at multiple scales, following the approach in Riemann et al. That means we need to do the following:\nSo to get started, we need to block our study area. We can do this using the spatial_block_cv() function from spatialsample. We’ll generate ten different sets of hexagon tiles, using cellsize arguments of between 10,000 and 100,000 meters3. The code to do that, and to store all of our resamples in a single tibble, looks like this4:\nset.seed(123)\nlibrary(dplyr)\nlibrary(spatialsample)\ncellsize &lt;- seq(10, 100, 10) * 1000\n\nriemann_resamples &lt;- tibble(\n  resamples = purrr::map(\n    cellsize, \n    \\(cs) {\n      spatial_block_cv(\n        plot_measurements,\n        v = Inf,\n        cellsize = cs,\n        square = FALSE\n      )\n    }\n  ),\n  cellsize = cellsize\n)\nIf we want, we can visualize one (or more) of our resamples, to get a sense of what our tiling looks like:\nautoplot(riemann_resamples$resamples[[10]])\nAnd that’s step 1 of the process completed! Now we need to move on to step 2, and actually fit models to each of these resamples. I want to highlight that this is a lot of models, and so is going to take a while5:\npurrr::map_dbl(\n  riemann_resamples$resamples,\n  nrow\n) |&gt; \n  sum()\n\n[1] 2600\nWith that said, actually fitting those few thousand models is a two part process. First, we’re going to load the rest of the tidymodels packages and use them to define a workflow (from the workflows package), specifying the formula and model that we want to fit to each resample:\nlibrary(tidymodels)\n\nlm_workflow &lt;- workflow() |&gt; \n  add_model(linear_reg()) |&gt; \n  add_formula(agb ~ n_trees)\nNext, we’ll actually apply that workflow a few thousand times! We’ll calculate two metrics for each run of the model: the root-mean-squared error (RMSE) and the mean absolute error (MAE). We can add these metrics as a new column to our resamples using the following:\nriemann_resamples &lt;- riemann_resamples |&gt; \n  mutate(\n    resampled_outputs = purrr::map(\n      resamples, \n      fit_resamples,\n      object = lm_workflow,\n      metrics = metric_set(\n        rmse,\n        mae\n      )\n    )\n  )\nThe riemann_resamples object now includes both our original resamples as well as the accuracy metrics associated with each run of the model. A very cool thing about this approach is that we can now visualize our block-level accuracy metrics with a few lines of code.\nFor instance, if we wanted to plot block-level RMSE for our largest assessment scale, we could use the following code to “unnest” our nested metric and resample columns:\nriemann_resamples$resampled_outputs[[10]] |&gt; \n  mutate(splits = purrr::map(splits, assessment)) |&gt; \n  unnest(.metrics) |&gt; \n  filter(.metric == \"rmse\") |&gt; \n  unnest(splits) |&gt; \n  st_as_sf() |&gt; \n  ggplot(aes(color = .estimate)) + \n  geom_sf()\nWe can also go on to the third step of our assessment process, and get our model accuracy metrics for each aggregation scale we investigated. We’ll create a new data frame with only our cellsize variable and the associated model metrics:\nriemann_metrics &lt;- riemann_resamples |&gt; \n  transmute(\n    cellsize = cellsize,\n    resampled_metrics = purrr::map(resampled_outputs, collect_metrics)\n  ) |&gt; \n  unnest(resampled_metrics)\n\nhead(riemann_metrics)\n\n# A tibble: 6 × 7\n  cellsize .metric .estimator  mean     n std_err .config             \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1    10000 mae     standard   5787.  1541    99.9 Preprocessor1_Model1\n2    10000 rmse    standard   6980.  1541   121.  Preprocessor1_Model1\n3    20000 mae     standard   5722.   424   130.  Preprocessor1_Model1\n4    20000 rmse    standard   7644.   424   169.  Preprocessor1_Model1\n5    30000 mae     standard   5637.   205   161.  Preprocessor1_Model1\n6    30000 rmse    standard   7725.   205   218.  Preprocessor1_Model1\nAnd just like that, we’ve got a multi-scale assessment of our model’s accuracy! We can then use this to investigate and report how well our model does at different levels of aggregation. For instance, by plotting RMSE against MAE at various scales, it appears that our RMSE increases with aggregation while MAE decreases. This hints that, as we aggregate our predictions to larger hexagons, more of our model’s overall error is driven by large outliers:\nlibrary(ggplot2)\n\nggplot(riemann_metrics, aes(cellsize, mean, color = .metric)) + \n  geom_line() +\n  geom_point() + \n  theme_minimal()"
  },
  {
    "objectID": "junkdrawer/riemann/index.html#footnotes",
    "href": "junkdrawer/riemann/index.html#footnotes",
    "title": "Multi-scale model assessment with spatialsample",
    "section": "Footnotes",
    "text": "Footnotes\n\nRiemann, R., Wilston, B. T., Lister, A., and Parks, S. 2010. An effective assessment protocol for continuous geospatial datasets of forest characteristics using USFS Forest Inventory and Analysis (FIA) data. Remote Sensing of Environment, 114, pp. 2337-2353. doi: 10.1016/j.rse.2010.05.010.↩︎\nAmong them that only forested areas are measured, where “forested” means “principally used as forest” which excludes parks but includes recently clear-cut lands, and that plot locations are considered personally identifying information under the farm bill of 1985, and so as to not identify anyone the coordinates in public data are “fuzzed” by a few miles and approximately 20% of plot coordinates are swapped with other plots in the data set. Which is to say, consult your local forester or ecologist if you want to use FIA data to answer real questions in your own work.↩︎\nThis value is in meters because our coordinate reference system is in meters. It represents the length of the apothem, from the center of each polygon to the middle of the side. We’re using hexagons because Riemann et al. also used hexagons.↩︎\nv is Inf because we want to perform leave-one-block-out cross-validation, but we don’t know how many blocks there will be before they’re created. This is the supported way to do leave-one-X-out cross-validation in spatialsample &gt; 0.2.0 (another option is to set v = NULL).↩︎\nLinear regression was invented in 1805, ish, long before the Analytical Engine was a twinkle in Babbage’s eye. Whenever I get frustrated at how long fitting multiple models like this takes, I like to take a step back and recognize that I am asking my poor overworked computer to fit roughly as many models as were used in the first ~100 years of the technique’s life.↩︎"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html",
    "href": "junkdrawer/summer2021/index.html",
    "title": "Summer 2021 Targets",
    "section": "",
    "text": "Otherwise, unless we have more observations or are willing to compute more predictors, I think this is pretty well set\n\n\n\n\n\nHistoric AGB map accuracy pipeline\nI think it makes sense for me to start getting more involved with this end, & give historical models the attention I’ve given the LiDAR-year set. But I haven’t been particularly involved before, I don’t know what this entails beyond:\n\nWork with non-WWE sets\nInclude more predictors\nEnsembling\n\n\n\n\n\n\nBreak up model building notebook into functions; probably its own package\n\n\n\n\n\nCarbon Accounting Tool"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#cafri",
    "href": "junkdrawer/summer2021/index.html#cafri",
    "title": "Summer 2021 Targets",
    "section": "",
    "text": "Otherwise, unless we have more observations or are willing to compute more predictors, I think this is pretty well set\n\n\n\n\n\nHistoric AGB map accuracy pipeline\nI think it makes sense for me to start getting more involved with this end, & give historical models the attention I’ve given the LiDAR-year set. But I haven’t been particularly involved before, I don’t know what this entails beyond:\n\nWork with non-WWE sets\nInclude more predictors\nEnsembling\n\n\n\n\n\n\nBreak up model building notebook into functions; probably its own package\n\n\n\n\n\nCarbon Accounting Tool"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#dissertation-research",
    "href": "junkdrawer/summer2021/index.html#dissertation-research",
    "title": "Summer 2021 Targets",
    "section": "Dissertation Research",
    "text": "Dissertation Research\n\nWrite proposal\nterrain-into-Unity pipeline\nPlace arbitrary object pipeline\nStretch goal: chapter 1 (On Abstraction)\nStretch goal: chapter 2 (survey of environmental visualization)"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#ml-course",
    "href": "junkdrawer/summer2021/index.html#ml-course",
    "title": "Summer 2021 Targets",
    "section": "ML Course",
    "text": "ML Course\n\nWrite course handouts\nWrite assignments"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#personal",
    "href": "junkdrawer/summer2021/index.html#personal",
    "title": "Summer 2021 Targets",
    "section": "Personal",
    "text": "Personal\n\nNo plans yet but going to try to disappear into the woods for a week if I can swing it"
  },
  {
    "objectID": "junkdrawer/summer2021/index.html#completed",
    "href": "junkdrawer/summer2021/index.html#completed",
    "title": "Summer 2021 Targets",
    "section": "Completed",
    "text": "Completed\n\nLiDAR-Year AGB\n\nAdd in the missing 30ish plots"
  },
  {
    "objectID": "vr/index.html",
    "href": "vr/index.html",
    "title": "Virtual Reality is Always Coming Never",
    "section": "",
    "text": "Advances in graphical technology have now made it possible for us to interact with information in innovative ways, most notably by exploring multimedia environments and by manipulating three-dimensional virtual worlds.\n— Mike Scaife & Yvonne Rogers: External cognition: how do graphical representations work?"
  },
  {
    "objectID": "vr/index.html#section",
    "href": "vr/index.html#section",
    "title": "Virtual Reality is Always Coming Never",
    "section": "",
    "text": "Advances in graphical technology have now made it possible for us to interact with information in innovative ways, most notably by exploring multimedia environments and by manipulating three-dimensional virtual worlds.\n— Mike Scaife & Yvonne Rogers: External cognition: how do graphical representations work?"
  },
  {
    "objectID": "posts/2021/03/index.html",
    "href": "posts/2021/03/index.html",
    "title": "Central coast BC weather station service",
    "section": "",
    "text": "Here’s Ali, Griffin and I completing a snow course at the Klinaklina weather station on the central coast. Check out the real-time weather station data for this site Here."
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Conference Presentations",
    "section": "",
    "text": "Cebulski, A. C. and Pomeroy, J. W. (2023). The Influence of Meteorology on Snow Interception Processes. American Geophysical Union Meeting. San Francisco, California, United States of America (Poster, regional, PhD work).\nCebulski, A. C. and Pomeroy, J. W. (2023). New Observations of Snow Interception Processes in a Windswept Subalpine Environment. Global Water Futures Annual Science Meeting. Saskatoon, Saskatchewan, Canada (Poster, regional, PhD work).\nCebulski, A. C. and Pomeroy, J. W. (2023). Snow Interception Processes and Prediction in a Windswept Subalpine Environment. Canadian Geophysical Union. Banff, Alberta, Canada (Oral, regional, PhD work).\nCebulski, A. C. and Pomeroy, J. W. (2022). Snow Interception Processes and Prediction in a Windswept Subalpine Environment. International Glaciological Society - International Symposium on Snow. Davos, Switzerland. (Oral, international, PhD work).\nButman, D., McNicol, G., Giesbrecht, I., Tank, S., Hood, E. W., Floyd, B. C., Lung, J. L., D’Amore, D., McSorely, H., Cebulski, A. C., Trubilowicz, J. W., Oliver, A., Fylpaa, C., Fellman, J., Frazer, G. W., Lally, A. S., and Arriola, S. G. (2020). Carbon Export from the Pacific Northwest Coastal Rainforest Margin. American Geophysical Union Meeting. Online. (Poster, international, data analyst work at VIU).\nCebulski, A. C., Floyd, B. C., Trubilowicz, J. W., D’Amore, D., Bidlack, A. (2019). Quantifying freshwater discharge into the Pacific Ocean from southeast Alaska to northern California using a distributed hydrological model. MABRRI Regional Research Conference. Parksville, BC. (Oral, regional, data analyst work at VIU)\nCebulski, A. C., Desloges, J. R. (2018). The Glaciolacustrine Sediment Record of Cariboo Lake, BC: Implications for Holocene Fluvial and Glacial Watershed Dynamics. Joint meeting of the Canadian and American Quaternary Associations. Ottawa, ON. (Oral, international, MSc work)\nCebulski, A. C., Enanga, E., Creed, I (2016). Quantifying the Carbon Sequestration of Restored Wetlands in the Agricultural Landscape of Southern Ontario. World Wetland Day Conference. Waterloo, ON. (Poster, international, undergraduate work)"
  }
]