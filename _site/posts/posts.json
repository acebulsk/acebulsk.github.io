[
  {
    "path": "posts/2021-06-14-interactive-landscape-simulations-for-visual-resource-assessment/",
    "title": "Interactive landscape simulations for visual resource assessment",
    "description": "New preprint for the 2021 Visual Resources Stewardship Conference.",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2021-06-14",
    "categories": [],
    "contents": "\nI’ll be speaking at the 2021 Visual Resources Stewardship Conference, presenting a paper (“Interactive landscape simulations for visual resource assessment”, with Colin Beier and Aidan Ackerman) talking about how the use of game engines for spatial visualization can help in visual resource assessment projects.\n\n\n\nFigure 1: Everything the light touches can see & be seen by the red dot. Users can interactively walk around the landscape to see for themselves if the viewshed algorithm gets things exactly right.\n\n\n\nThis was a really fun project, forcing me to push these visualizations in a new direction. The conference itself is in October, and I’ve posted a preprint of the paper at this link.\n[Update - 2021-06-24] And the code and manuscript used for this paper are now on Github at this link.\n\n\n\n",
    "preview": "posts/2021-06-14-interactive-landscape-simulations-for-visual-resource-assessment/closer.jpg",
    "last_modified": "2021-06-24T14:24:39-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-12-virtual-environments-talk-at-user-2021/",
    "title": "Virtual Environments talk at useR! 2021",
    "description": "With slides and a rough outline available on GitHub.",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2021-06-12",
    "categories": [],
    "contents": "\nI’m thrilled to be giving a talk (with Colin Beier and Aidan Ackerman) at useR 2021! We’re calling the talk “Virtual Environments: Using R as a Frontend for 3D Rendering of Digital Landscapes” – generally speaking, I’m talking about the idea of using R to create visualizations inside game engines, and specifically about how we do so using terrainr.\nThe video should go up on YouTube after the conference, and the slides (and a rough script for the talk) are online at https://github.com/mikemahoney218/user2021 . My favorite line from the script is “R is where our users are” – I didn’t quite realize I had any Boston accent left until I tripped over that sentence a few times.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-12T12:20:46-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-22-terrainr-version-040-is-now-on-cran/",
    "title": "What's new in terrainr 0.4.0?",
    "description": "The new CRAN release of terrainr improves consistency, CRS logic, and \nfixes some bugs.",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2021-04-22",
    "categories": [],
    "contents": "\n\nContents\nWhat’s a terrainr?\nmerge_rasters can handle tiles with different numbers of bands\nget_tiles doesn’t auto-download transparency values for NAIP\nFunctions pay attention to the provided CRS\n\nterrainr version 0.4.0 is now on CRAN! This version is a relatively minor update that shouldn’t impact most workflows, but makes some changes to improve the logic and consistency of the package. The rest of this post runs through the changes you can expect if you update.packages() any time soon!\nWhat’s a terrainr?\nterrainr is an R package for the retrieval and visualization of spatial data. It provides functions to download elevation data and basemap tiles for points within the United States (using public domain data from the USGS National Map), visualize them in R via ggplot2, and process them for 3D visualization using the Unity 3D engine. You can see the GitHub repo here!\nmerge_rasters can handle tiles with different numbers of bands\nThe old implementation of merge_rasters was very bulky, read all your map tiles into memory at once, and was a bit of a mess to maintain thanks to the large number of paths you could theoretically take through the code. The commit (suggested via rOpenSci review!) replacing it with gdalwarp via sf is probably the single best code improvement I’ve made to this repo. Unfortunately, the old method could also handle merging rasters with differing numbers of bands, while the simple gdalwarp fix couldn’t.\nSo the old implementation is back as an internal method while I look for a better solution to this problem. merge_rasters will now attempt to use gdalwarp to merge your input files and then fall back to (a massively simplified version of) the older version if gdalwarp fails.\nAs for why you’d want to automatically merge rasters with different numbers of bands, well…\nget_tiles doesn’t auto-download transparency values for NAIP\nNAIP orthophotography provides fantastic continuous 1-meter images for the continental United States. When downloading these photos with the argument transparency = true, which used to be the default, most photos don’t have any transparent pixels to talk about and as such are returned and saved as 3 band rasters (RGB images). Some photos, however, do have such pixels and are returned with a 4th alpha band. This causes problems with gdalwarp as well as image editing software, and the majority of the time users are not better served by these pixels being transparent.\nAs a result, this version changes the default transparency argument for get_tiles and hit_national_map_api to false when downloading NAIP images (no other data source is impacted). This is one of the reasons this version gets a 0.x.0 number – while it should be a small change, the same inputs to functions no longer returns the same outputs (though I doubt people would notice), so I’m counting this as a breaking change.\nThere’s a slightly more impactful breaking change worth noting though:\nFunctions pay attention to the provided CRS\nThis header is actually about two distinct changes.\nFirst, another new behavior with get_tiles is that rather than assuming the provided data and downloaded image should both be using the WGS84 CRS (EPSG 4326), get_tiles will now infer the EPSG CRS from any provided sf or Raster object. If the numeric code is missing, this function will still assume 4326.\nSimilarly, rather than specifying target_crs in vector_to_overlay, this function will now return an overlay projected to match reference_raster’s CRS. Missing CRS are handled slightly differently here – if the error_crs argument is NULL, this function will warn; if FALSE it will assume 4326, and if TRUE it will interrupt the function with an error.\nThose are the major changes to this iteration! On top of these there are some minor changes to the package internals, slowly removing dead code paths and simplifying things behind the scenes. If you have any problems (bugs or missing features) with the package, feel free to open an issue!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-23T08:05:24-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021/02/terrainr/",
    "title": "terrainr 0.3.0 is out today",
    "description": "New version, who this?",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2021-02-17",
    "categories": [
      "R",
      "Data science",
      "terrainr"
    ],
    "contents": "\n\nContents\nBreaking Changes\nObject Classes Are Dead; Long Live Object Classes\nNew Names, Who This?\nFewer Utilities, More Useful\n\nShow Me What You Got\nNew Docs, Who This?\n\nterrainr is in review with rOpenSci and the first review just came back! I’ve been working through the comments over the past week or so, and today that work has culminated in the release of terrainr version 0.3.0.\nThis is a big release with a handful of breaking changes, so I felt like I should give a brief overview of the biggest user-facing changes.\nBreaking Changes\nObject Classes Are Dead; Long Live Object Classes\nThe single largest change is that terrainr specific classes are no longer exported, and users shouldn’t need to worry about getting data into or out of those formats anymore. Instead, use any sf or Raster object in their place. For instance, workflows that used to look like this:\n\n\n# Doesn't run:\nlibrary(terrainr)\nsimulated_data <-  data.frame(id = seq(1, 100, 1),\n                              lat = runif(100, 44.04905, 44.17609), \n                              lng = runif(100, -74.01188, -73.83493))\n\nbbox <- get_bbox(lat = simulated_data$lat, lng = simulated_data$lng) \n\noutput_tiles <- get_tiles(bbox = bbox,\n                          services = c(\"elevation\", \"ortho\"),\n                          resolution = 90)\n\n\n\nNow look like this:\n\n\nlibrary(terrainr)\nsimulated_data <-  data.frame(id = seq(1, 100, 1),\n                              lat = runif(100, 44.04905, 44.17609), \n                              lng = runif(100, -74.01188, -73.83493))\n\nsimulated_data <- sf::st_as_sf(simulated_data, coords = c(\"lng\", \"lat\"))\nsimulated_data <- sf::st_set_crs(simulated_data, 4326)\n\noutput_tiles <- get_tiles(data = simulated_data,\n                          services = c(\"elevation\", \"ortho\"),\n                          resolution = 90)\n\n\n\nAs part of this change, get_bbox, get_coordinate_bbox, and all class creation and export functions are gone now. Use sf (or Raster*) objects in their place instead.\nNew Names, Who This?\nget_tiles now uses the services argument to name its output list:\n\n\nnames(output_tiles)\n\n\n[1] \"elevation\" \"ortho\"    \n\nThis means that if you request the service elevation you can retrieve your tiles using the name elevation. If you request the same endpoint with multiple names, get_tiles will use whatever name was first in the vector.\nFewer Utilities, More Useful\nUtility functions calc_haversine_distance, convert_distance, point_from_distance, rad_to_deg, and deg_to_rad have been removed (or removed from exports). For unit conversions, check out the units package. This shouldn’t impact the main uses of the package, but is still worth flagging.\nShow Me What You Got\nterrainr 0.3.0 adds a ggplot2 geom, geom_spatial_rgb, for plotting 3 band RGB rasters:\n\n\nlibrary(ggplot2)\n\nggplot() + \n  geom_spatial_rgb(data = output_tiles[[\"ortho\"]],\n                   # Required aesthetics r/g/b specify color bands:\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326)\n\n\n\n\nNote that above we just passed the file path to our raster; we can also pass a RasterStack:\n\n\northo <- raster::stack(output_tiles[[\"ortho\"]])\n\nggplot() + \n  geom_spatial_rgb(data = ortho,\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326)\n\n\n\n\nOr a data.frame:\n\n\northo_df <- raster::as.data.frame(ortho, xy = TRUE)\nnames(ortho_df) <- c(\"x\", \"y\", \"red\", \"green\", \"blue\")\n\nggplot() + \n  geom_spatial_rgb(data = ortho,\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  coord_sf(crs = 4326)\n\n\n\n\nNote that each step here gives you a little more control over the output image – for instance, if your raster bands aren’t in RGB order (or you have more than RGBA bands), you’ll need to provide a data.frame to get a true color image.\nYou can then use these basemaps like most other ggplot geoms:\n\n\nggplot() + \n  geom_spatial_rgb(data = ortho_df,\n                   aes(x = x, y = y, r = red, g = green, b = blue)) + \n  geom_sf(data = simulated_data) + \n  coord_sf(crs = 4326)\n\n\n\n\nNew Docs, Who This?\nThose are just a few of the changes in 0.3.0; you can find a longer list in the NEWS file.\nOne thing not mentioned in the NEWS file, though, is that this version of terrainr included a complete rewrite of the documentation. The docs were mostly written while the package was being conceptually developed, and as a result gave a bit too much emphasis to some ideas while completely ignoring others. So I’ve rewritten all of the documentation that lives on the terrainr website – let me know what you think about the new versions (or if you catch anything I’ve missed!).\n\n\n\n",
    "preview": "posts/2021/02/terrainr/terrainr_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021/01/model-averaging/",
    "title": "Model averaging methods: how and why to build ensemble models",
    "description": "Averaging predictions for fun and profit -- and for dealing with the uncertainty of model selection. With examples in R!",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2021-01-18",
    "categories": [
      "R",
      "Data science"
    ],
    "contents": "\n\nContents\nIntroduction\nGetting Started\n\nComponent Models\nLinear Model\nRandom Forest\nGBM\n\nModel Averaging\nEqual Weights\nFit-Based Weights\nModel-Based Weights\n\nHow’d We Do?\nConclusion\n\n\n\n\n\nCartoon components are CC-BY 4.0 Allison Horst (allison_horst on twitter)\nIntroduction\nBuilding models is hard. Choosing what models to build can be even harder. With seemingly infinite different modeling approaches to select between (and somehow even more individual implementations), it can be difficult to guess what methods will be the best fit for your data – particularly if you’re working with data that will change over time with new observations or predictors being added to the mix.\nUsually, we disclose this sort of uncertainty with things like confidence intervals and standard errors. Yet when it comes to selecting a single model, we often don’t discuss how confident we are in that model being the right one – instead, we present and report only our final choice as if there was no chance other candidate models would be as good or even better fits.\nEnsemble models prove a way to deal with that uncertainty (Wintle et al. 2003). By averaging predictions from a handful of candidate models, ensembles acknowledge that there might be multiple models that could be used to describe our data – and by weighting the average we can communicate how confident we are in each individual model’s view of the world. Of course, while this is all nice and flowery, it needs to work too – and model averaging delivers, typically reducing prediction errors beyond even above even the best individual component model (Dormann et al. 2018).\nThere are a ton of approaches to model averaging1. The rest of this post will walk through a few of the simplest – equal-weight averaging, fit-based averages, and model-based combinations – that you can easily implement yourself without needing to worry about slowing down your iteration time or making your modeling code too complex.\nGetting Started\nWe’ll be using the following libraries for data manipulation and visualization:\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(knitr)\n\n\n\nAdditionally, we’ll be using both ranger and lightgbm to develop component models:\n\n\nlibrary(ranger)\nlibrary(lightgbm)\n\n\n\nAnd finally, we need the actual data we’re modeling. For this example, we’ll build models predicting the arrival delay of the flights included in the nycflights13 package using both flight details and weather data. This next chunk of code will preprocess our data into a model-ready format:\n\n\nflights <- nycflights13::flights\nweather <- nycflights13::weather %>% \n  select(-wind_gust) # About 80% missing values, so we'll drop this column\n\n# combine the two data frames into one complete set\nflight_data <- flights %>% \n  left_join(weather,\n            by = c(\"year\", \"month\", \"day\", \"origin\", \"hour\", \"time_hour\")) %>% \n  drop_na()\n\nflight_data <- flight_data %>% \n  # Drop 37 pretty dramatic outliers\n  filter(arr_delay <= 500) %>% \n  # Get rid of useless predictors -- \n  # these each cause problems with at least one of our regressions\n  select(-year, -time_hour, -minute) %>% \n  # Skip the work of encoding non-numeric values, to save my poor laptop\n  select_if(is.numeric)\n\n\n\nAnd for one final pre-processing step, we’ll split our data into training, validation, and testing sets (sticking 20% into both validation and testing and dumping the rest into training). We’ll be using model performance against the validation set to determine weights for our averages.\n\n\nset.seed(123)\n# Generate a random sequence to subset our data into train/validate/test splits\nrow_index <- sample(nrow(flight_data), nrow(flight_data))\n\n# Testing gets the 20% of data with the highest random index values\nflight_testing <- flight_data[row_index >= nrow(flight_data) * 0.8, ]\n\n# Validation gets the next highest 20%\nflight_validation <- flight_data[row_index >= nrow(flight_data) * 0.6 &\n                                   row_index < nrow(flight_data) * 0.8, ]\n\n# Training gets the rest\nflight_training <- flight_data[row_index < nrow(flight_data) * 0.6, ]\n\n# LightGBM requires matrices, rather than data frames and formulas:\nxtrain <- as.matrix(select(flight_training, -arr_delay))\nytrain <- as.matrix(flight_training[[\"arr_delay\"]])\n\nxvalid <- as.matrix(select(flight_validation, -arr_delay))\nxtest <- as.matrix(select(flight_testing, -arr_delay))\n\n\n\nSo with that out of the way, it’s time to start training our models!\nComponent Models\nLinear Model\nLet’s start off with a simple linear regression model, using all of our predictors in the flight dataset to try and estimate arrival delays:\n\n\nlinear_model <- lm(arr_delay ~ ., flight_training)\nsummary(linear_model)\n\n\n\nCall:\nlm(formula = arr_delay ~ ., data = flight_training)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.895  -9.133  -1.538   7.076 159.388 \n\nCoefficients:\n                 Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)    -4.613e+00  5.992e+00   -0.770 0.441394    \nmonth           3.825e-02  1.117e-02    3.424 0.000618 ***\nday             2.220e-02  4.109e-03    5.404 6.51e-08 ***\ndep_time        9.509e-05  2.795e-04    0.340 0.733722    \nsched_dep_time -3.492e-03  1.894e-03   -1.844 0.065257 .  \ndep_delay       1.013e+00  1.068e-03  948.332  < 2e-16 ***\narr_time        8.816e-04  1.182e-04    7.460 8.68e-14 ***\nsched_arr_time -4.713e-03  1.478e-04  -31.884  < 2e-16 ***\nflight         -4.692e-05  2.541e-05   -1.846 0.064863 .  \nair_time        7.563e-01  3.074e-03  246.006  < 2e-16 ***\ndistance       -9.792e-02  3.925e-04 -249.500  < 2e-16 ***\nhour            6.000e-01  1.871e-01    3.207 0.001341 ** \ntemp            1.173e-01  2.232e-02    5.254 1.49e-07 ***\ndewp            3.632e-02  2.405e-02    1.510 0.130928    \nhumid           1.860e-02  1.229e-02    1.514 0.130053    \nwind_dir       -6.076e-03  4.009e-04  -15.158  < 2e-16 ***\nwind_speed      1.920e-01  7.538e-03   25.471  < 2e-16 ***\nprecip          2.688e+01  3.014e+00    8.920  < 2e-16 ***\npressure       -1.634e-02  5.619e-03   -2.909 0.003631 ** \nvisib          -4.603e-01  3.239e-02  -14.212  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.82 on 170687 degrees of freedom\nMultiple R-squared:  0.8707,    Adjusted R-squared:  0.8707 \nF-statistic: 6.048e+04 on 19 and 170687 DF,  p-value: < 2.2e-16\n\nCool! We have our first model – and it seems to be a pretty ok fit, with an R^2 of 0.87. We could probably make this model better by being a bit more selective with our terms or throwing in some interaction terms – but as a first stab at a model that we’ll incorporate into our average, this is pretty alright.\nOf course, we want to make sure this model can generalize outside of the data it was trained with – let’s use it to make predictions against our validation set, too:\n\n\nflight_validation$lm_pred <- predict(\n  linear_model,\n  newdata = flight_validation\n)\n\nsqrt(mean((flight_validation$lm_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ lm_pred, flight_validation))$r.squared\n\n\n\n\n[1] 14.73962\n[1] 0.8684178\n\nR^2 remains at about 0.87 and RMSE comes in at about 14.74 minutes – which, considering delays in the validation set range from -75 to 485 minutes, feels not too bad for a naively implemented linear model.\nRandom Forest\nSo we have our first model sorted, but we need more than that to take an average! While we could average out a number of linear models with different parameters, it feels more interesting to combine a few different algorithms as component models. So let’s use ranger to implement a random forest to represent our data – fair warning, this one takes a little while to train!\n\n\nranger_model <- ranger::ranger(arr_delay ~ ., data = flight_training)\nsqrt(ranger_model$prediction.error)\nranger_model$r.squared\n\n\n\n\n[1] 11.08573\n[1] 0.9276561\n\nSo this model has an RMSE of 11.09 and an R^2 of 0.93 – an improvement over our linear model! While we could eke out some improvements with careful tuning, it looks like this version is a good enough fit to use as an example in our ensemble. As before, we want to check out how well this model generalizes by using it to generate predictions for our validation set:\n\n\nranger_predictions <- predict(\n    ranger_model,\n    data = flight_validation\n  )\n\nflight_validation$ranger_pred <- ranger_predictions$predictions\n\nsqrt(mean((flight_validation$ranger_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ ranger_pred, flight_validation))$r.squared\n\n\n\n\n[1] 10.96209\n[1] 0.9302306\n\nOur model actually performs (extremely) slightly better on the validation set!\nGBM\nSo that’s two models sorted! For completeness sake, let’s implement a third and final component model, this time using the LightGBM package to fit a gradient boosting machine. Similar to the last two, we won’t do a ton to parameterize this model – the only change I’ll make to the model fit defaults is to use 100 rounds, to let the boosting algorithm get into the same performance range as our other two models.\n\n\nlightgbm_model <- lightgbm::lightgbm(xtrain, \n                                     ytrain, \n                                     nrounds = 100, \n                                     obj = \"regression\", \n                                     metric = \"rmse\",\n                                     # Suppress output\n                                     force_col_wise = TRUE,\n                                     verbose = 0L)\n\n\n\nThe lightgbm_model doesn’t have the same easy method for evaluating in-bag performance as our linear model and random forests did. We’ll skip right to the validation set instead:\n\n\nflight_validation$lightgbm_pred <- predict(\n  lightgbm_model,\n  xvalid\n)\n\nsqrt(mean((flight_validation$lightgbm_pred - flight_validation$arr_delay)^2))\nsummary(lm(arr_delay ~ lightgbm_pred, flight_validation))$r.squared\n\n\n\n\n[1] 10.4088\n[1] 0.9347398\n\nSo it looks like LightGBM model performs about as well (if not marginally better than) our random forest! For reference, here are the RMSE values from each of our candidate models:\n\n\nprediction_values <- flight_validation %>% \n  # Only select our y and y-hat columns\n  select(ends_with(\"pred\"), matches(\"arr_delay\"))\n\nprediction_plots <- prediction_values %>% \n  pivot_longer(cols = -arr_delay) %>% \n  mutate(name = regmatches(name, regexpr(\".*(?=_pred)\", name, perl = TRUE)),\n         resid = value - arr_delay,\n         name = factor(name, levels = c(\"lightgbm\", \"ranger\", \"lm\")))\n\nprediction_plots %>% \n  group_by(Model = name) %>% \n  summarise(RMSE = sqrt(mean(resid^2)), .groups = \"drop\") %>% \n  arrange(RMSE) %>% \n  knitr::kable()\n\n\nModel\nRMSE\nlightgbm\n10.40880\nranger\n10.96209\nlm\n14.73962\n\nOf course, individual metrics don’t tell the whole story – it can be helpful to look at diagnostic plots of our predictions to try and understand patterns in how our predictions match the data. For instance, “linear models are about four minutes worse on average” is all well and good in the abstract, but graphics like the one below can help us see that – for instance – linear models tend to do a bit worse around 0 minute delays (where most of the data is clustered) while our random forest performs worse on higher extremes:\n\n\nprediction_plots %>% \n  ggplot(aes(value, arr_delay)) + \n  geom_point(alpha = 0.05) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") + \n  facet_wrap(~ name) + \n  theme_minimal() + \n  labs(x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\nModel Averaging\nWith our candidate models in tow, we’re now fully ready to move on to model averaging methods! We’ll walk through basic implementations of three methods (equal weighting, fit-based weights, and model-based estimates) and then evaluate our ensembles at the end.\nEqual Weights\nPerhaps the most obvious way to average models is to take the simple arithmetic mean of your model predictions. This method presupposes that each of your models are equally good representations of your underlying data; since that isn’t the case here, we might expect this method to not substantially reduce error overall.\nA benefit of this method, though, is that implementation takes no time at all:\n\n\nprediction_values <- prediction_values %>% \n  mutate(equal_weight_pred = (lm_pred + ranger_pred + lightgbm_pred) / 3)\n\n\n\nFit-Based Weights\nA slightly more involved method is to weight models based on some metric of their performance. Almost any metric with a standard definition across component models can be used (so for instance, AIC or BIC with nested models or MSE and MAPE); as we’ve been using RMSE so far, we’ll use it to weight our errors.\nWeighting models based on fit statistics is also relatively easy in the grand scheme of data science. First, calculate the fit statistic for each of your models:\n\n\nmodel_rmse <- vapply(\n  prediction_values,\n  function(x) sqrt(mean((x - prediction_values$arr_delay)^2)),\n  numeric(1)\n  )[1:3] # Only our 3 component models!\nmodel_rmse\n\n\n      lm_pred   ranger_pred lightgbm_pred \n     14.73962      10.96209      10.40880 \n\nThen, depending on your statistic, you may need to take the reciprocal of each value – as lower RMSEs are better, we need to do so here:\n\n\nrmse_weights <- (1 / (model_rmse))\n\n\n\nLastly, calculate your weights as proportion of the whole set of – you can view these values as the proportion of the ensemble prediction contributed by each component:\n\n\nrmse_weights <- rmse_weights / sum(rmse_weights)\nrmse_weights\n\n\n      lm_pred   ranger_pred lightgbm_pred \n    0.2659099     0.3575422     0.3765479 \n\nMaking predictions with the ensemble is then relatively easy – just multiply each of your predicted values by their proportion and sum the results:\n\n\nprediction_values <- prediction_values %>% \n  mutate(fit_based_pred = ((lm_pred * rmse_weights[\"lm_pred\"]) + \n                             (ranger_pred * rmse_weights[\"ranger_pred\"]) + \n                             (lightgbm_pred * rmse_weights[\"lightgbm_pred\"])))\n\n\n\nModel-Based Weights\nThe last averaging method we’ll walk through is a little more involved, but still pretty comprehensible: take your model outputs, turn around, and use them as model inputs.\n\nOur toy example here is a pretty good fit for this method – we already saw in our graphics that a strong linear relationship exists between our predictions and the true value, and this relationship is a little different for each model:\n\n\n\nFrom this plot, we can guess that a linear model combining our component predictions as features will be a good fit2 for averaging these models. To do so, we simply need to build a linear model:\n\n\npredictions_model <- lm(arr_delay ~ lm_pred * ranger_pred * lightgbm_pred, \n                        data = prediction_values)\n\n\n\nAnd then use it to generate predictions just like our original component linear model:\n\n\nprediction_values$model_based_pred <- predict(\n  predictions_model,\n  newdata = prediction_values\n)\n\n\n\nNote that if we saw non-linear relationships between our predictions and true values, we’d want to rely on non-linear methods to average out predictions; it just so happens that our models are already pretty strong fits for the underlying data and can be well-represented with simple linear regression.\nHow’d We Do?\nNow that we have our ensemble models prepared, it’s time to evaluate all of our models out against our testing set!\nThe first step is to generate predictions for the test set using our component models:\n\n\nflight_testing$lm_pred <- predict(\n  linear_model,\n  newdata = flight_testing\n)\n\nranger_predictions <- predict(\n    ranger_model,\n    data = flight_testing\n  )\n\nflight_testing$ranger_pred <- ranger_predictions$predictions\n\nflight_testing$lightgbm_pred <- predict(\n  lightgbm_model,\n  xtest\n)\n\n\n\nWe can use those predictions to generate our ensemble predictions. Note that we’re still using the weights and models calibrated on the validation data – we (theoretically) shouldn’t know the “true” values for the test set, so we can’t re-weight our averages now!\n\n\nflight_testing <- flight_testing %>% \n  mutate(equal_weight_pred = (lm_pred + ranger_pred + lightgbm_pred) / 3)\n\nflight_testing <- flight_testing %>% \n  mutate(fit_based_pred = ((lm_pred * rmse_weights[\"lm_pred\"]) + \n                             (ranger_pred * rmse_weights[\"ranger_pred\"]) + \n                             (lightgbm_pred * rmse_weights[\"lightgbm_pred\"])))\n\nflight_testing$model_based_pred <- predict(\n  predictions_model,\n  newdata = flight_testing\n)\n\n\n\n\n\n\nSo how’d we do? Let’s check out the RMSE for each of our models:\n\n\nprediction_values <- flight_testing %>% \n  select(ends_with(\"pred\"), matches(\"arr_delay\"))\n\nprediction_plots <- prediction_values %>% \n  pivot_longer(cols = -arr_delay) %>% \n  mutate(name = regmatches(name, regexpr(\".*(?=_pred)\", name, perl = TRUE)),\n         resid = value - arr_delay,\n         name = factor(name, \n                       levels = c(\"lightgbm\", \"ranger\", \"lm\",\n                                  \"model_based\", \"fit_based\", \"equal_weight\")))\n\nprediction_plots %>% \n  group_by(Model = name) %>% \n  summarise(RMSE = sqrt(mean(resid^2)), .groups = \"drop\") %>% \n  arrange(RMSE) %>% \n  knitr::kable()\n\n\nModel\nRMSE\nmodel_based\n9.492409\nlightgbm\n10.290113\nranger\n10.968544\nfit_based\n11.057728\nequal_weight\n11.311836\nlm\n14.621943\n\n\n\nprediction_plots %>% \n  ggplot(aes(value, arr_delay)) + \n  geom_point(alpha = 0.05) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") + \n  facet_wrap(~ name) + \n  theme_minimal() + \n  labs(x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\nCool – our model-based ensemble actually performed better than any of the components! While the equal weight and fit-based averages were pretty middle-of-the-road, in other settings these methods can also help to reduce bias in predictions and produce estimates with less variance than any of the component models.\nConclusion\nModel averaging can be a powerful tool for reducing model bias and addressing the implicit uncertainty in attempting to pick the “best” model for a situation. While plenty of complex and computationally expensive approaches to averaging exist – and can greatly improve model performance – simpler ensemble methods can provide the same benefits without necessarily incurring the same costs.\n\n\n\nDormann, Carsten F., Justin M. Calabrese, Gurutzeta Guillera-Arroita, Eleni Matechou, Volker Bahn, Kamil Bartoń, Colin M. Beale, et al. 2018. “Model Averaging in Ecology: A Review of Bayesian, Information-Theoretic, and Tactical Approaches for Predictive Inference.” Ecological Monographs 88 (4): 485–504. https://doi.org/https://doi.org/10.1002/ecm.1309.\n\n\nWintle, B. A., M. A. McCarthy, C. T. Volinsky, and R. P. Kavanagh. 2003. “The Use of Bayesian Model Averaging to Better Represent Uncertainty in Ecological Models.” Conservation Biology 17 (6): 1579–90. https://doi.org/https://doi.org/10.1111/j.1523-1739.2003.00614.x.\n\n\nSee table 1 of Dormann et. al. for a partial list.↩︎\nNo pun intended.↩︎\n",
    "preview": "posts/2021/01/model-averaging/model_avg.jpg",
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020/10/",
    "title": "Some Updates",
    "description": "Where Do We Come From? What Are We? Where Are We Going?",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2020-10-16",
    "categories": [
      "R",
      "Twitter",
      "phd",
      "terrainr",
      "beaver"
    ],
    "contents": "\nIt’s been a busy few months! Rather than try to make separate update posts for everything I’ve been up to, here’s a list of the top bullets from my past four months.\nNew Digs and New Gigs\nI’ve left Boston and left Wayfair to take a PhD position at SUNY-ESF in Syracuse, working with Colin Beier and Aidan Ackerman to make 3D landscape visualizations as a way to improve the interpretability of ecological models and help democratize scientific outputs.\nPaper: Stem size selectivity is stronger than species preferences for beaver, a central place forager\nMy first first-authored paper is out in Forest Ecology and Management! I wrote a little about the process of this paper, from start to finish, on my extremely short-lived newsletter.\nTweetbots\nI’ve now got a small army of robots living in the corner of my apartment, tweeting out to their heart’s content. At the moment, I’ve got three retweet bots running off the original ecology_tweets codebase, namely [@ecology_tweets](https://twitter.com/ecology_tweets), [@rstats_tweets](https://twitter.com/rstats_tweets) (a more heavily-filtered alternative to the more popular rstatstweets bot), and [@30daymap_tweets](https://twitter.com/30daymap_tweets), built for the #30DayMapChallenge.\nMore interesting are the two GPT-2 “AI” tweetbots now running, including [@fortunes_teller](https://twitter.com/fortunes_teller) and [@fund_me_please_](https://twitter.com/fund_me_please_). The former is trained against a collection of fortunes packages from various *nix distros and the R fortunes package, while the latter was run against 150 GRFP personal statements and is now tweeting out some frankly bizarre applications of its own making.\n{terrainr}\nI’ve got a new R package out, the first real “product” from my PhD. {terrainr} wraps the USGS National Map family of APIs to help users download geospatial data for their areas of interest, and provides functionality to turn those files into tiles that can be imported into Unity for 3D landscape visualization:\n\n\n\nFigure 1: A 3D landscape visualization of the area southeast of Mt. Whitney, California, USA.\n\n\n\nI love writing packages with visual outputs. I love writing packages with visual outputs that look like this.\nMisc\nI bought a pen plotter.\n\n\n\n\n",
    "preview": "posts/2020/10/terrainr.jpg",
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020/06/",
    "title": "Make a Retweet Bot in R",
    "description": "Y'know. If you wanna.",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2020-06-20",
    "categories": [
      "R",
      "Twitter",
      "ecology_tweets"
    ],
    "contents": "\nA while back, I made a tweetbot that retweets a set of ecology-related hashtags, in order to signal boost ecology content in a similar manner to statsbot or Plotter Bot. The code to do this is pretty simple – made almost trivial by the rtweet package – but I found that environmental hashtags have a pretty low signal-to-noise ratio, driven down by various political and industry groups, as well as trolls.\nI didn’t want to get into the business of content filtering, so instead I started looking for other markers that a tweet was – or wasn’t – worth promoting, and have gotten to what I believe is a respectable place with my filtration. So I’ve open-sourced the code (without the specific values I use to filter) for anyone else who might be interested in setting up their own automated retweet app.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020/05/",
    "title": "Installing the TIG stack on Raspberry Pi",
    "description": "A guide to installing InfluxDB, Telegraf, and Grafana on a Raspberry Pi 4 running Raspbian Buster. Unlike every other guide like this on the internet, this one works.",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2020-05-03",
    "categories": [
      "Raspberry Pi",
      "Tutorials",
      "Data Visualization",
      "Monitoring",
      "Telegraf",
      "InfluxDB",
      "Grafana"
    ],
    "contents": "\nSetting Up InfluxDB, Telegraf, and Grafana on Raspberry Pi\n\n\n\ntl;dr\nDo the following in a shell you’ve already auth’d into sudo on:\nsudo apt update\nsudo apt upgrade\n\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -\n# change \"buster\" as appropriate for your distro\necho \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\nsudo apt update\nsudo apt install influxdb\nsudo systemctl unmask influxdb\nsudo systemctl enable influxdb\nsudo systemctl start influxdb\n\n# you can find the current telegraf release here: https://portal.influxdata.com/downloads/\nwget https://dl.influxdata.com/telegraf/releases/telegraf-1.14.2_linux_armhf.tar.gz\ntar xf telegraf-1.14.2_linux_armhf.tar.gz\nsudo systemctl enable --now telegraf\nrm telegraf-1.14.2_linux_armhf.tar.gz\n\nsudo apt-get install -y adduser libfontconfig1\n# you can find the current grafana release here: https://grafana.com/grafana/download\nwget https://dl.grafana.com/oss/release/grafana_6.7.3_armhf.deb\nsudo dpkg -i grafana_6.7.3_armhf.deb\nsudo systemctl enable --now grafana-server\nrm grafana_6.7.3_armhf.deb\n \n \nThis should cause all three services to start on system boot. You’ll need to configure Telegraf to actually write to your local Influx instance at http://127.0.0.1:8086 (there’s a sample config under the Telegraf part of the post), then set up Grafana to read from Influx (at the same port) via the UI at localhost:3000.\n\nSetting up the TIG stack on Raspberry Pi\nI’m getting a little cabin-fevery as the 2020 quarantine moves into its third month. To try and defray some of the extra energy, I’ve been hacking on a Pi I set up with a Pi-hole and openvpn server about a month ago.\nOne of the cool things about the Pi-hole is that it gives you a little at-a-glance view of how your machine is doing, including CPU load, memory utilization, and temperature. This window into system stats made me realize that my little box is packing heat:\n\nI’m running a Pi 4, which is known for generating more heat than it can handle, so temperatures of ~60 C (the upper range of “safe”) isn’t too shocking – but with summer coming and me planning to add some load to this machine in the near future, I wanted to set up monitoring to make sure my box wasn’t going to melt on me. This also has the side benefit that I’ll have a metrics system already in place for anything else I stand up on this machine.\nEnter the TIG stack. TIG – Telegraf, InfluxDB, and Grafana – is a suite of open-source solutions for collecting, storing, and visualizing time-series data, like the sort you’ll get from repeatedly measuring system temperature.\nThis tutorial will walk you through setting up each of these services separately. These steps were tested on a Raspberry Pi 4 running Raspbian Buster, so other configurations might require some tweaking.\nAll of the code here should be run in a terminal on your Raspberry Pi unless I specify it needs to go somewhere else. To make sure you’re not going to run into dependency hell, it’s a good idea to run sudo apt update && sudo apt upgrade before installing any of the stack.\n\nInfluxDB\nFirst up, we need to set up our InfluxDB instance. This database is where our Telegraf instance will send metrics and where Grafana will read from, so it makes sense to stand it up first!\nInstalling the service is easy enough – we just need to add Influx’s authentication key, add their repository to our trusted sources, and then install it via apt:\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -\n# change \"buster\" as appropriate for your distro\necho \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\nsudo apt update\nsudo apt install influxdb influxdb-client\n\n\nNow we want to actually start the database, and tell our system to start it after reboots – since we’re expecting to always be collecting metrics via Telegraf, we need to make sure that we always have a place to write to, as well. This is a quick two-liner using systemctl – we first need to unmask Influx, which will let us add it as a service, then tell our Pi to start the service both right now and every time the system restarts via the enable --now command:\nsudo systemctl unmask influxdb\nsudo systemctl enable --now influxdb\n \n \nAfter this, you should be able to run systemctl status influxdb to see the service status – if everything went according to plan, you should see Active: active (running) around line 3 of the output.\nAt this point, it’s probably healthy to add authentication to your Influx instance if your pi is exposed to external networks. You can set up a basic admin account via:\ninflux\nCREATE USER admin WITH PASSWORD '<password>' WITH ALL PRIVILEGES\n \n \nYou can then force HTTP authentication by adding the following under the HTTP header in /etc/influxdb/influxdb.conf:\n[HTTP]\nauth-enabled = true\npprof-enabled = true\npprof-auth-enabled = true\nping-auth-enabled = true\n \n \nThe changes take effect the next time your service starts, which you can trigger via sudo systemctl restart influxdb.\n\nTelegraf\nWith Influx up and running, it’s time for us to start writing records, which means standing up Telegraf!\nTelegraf is updated pretty frequently, so it’s a good idea to check the release page to see what version you should be installing. At the time of writing, the current version is 1.14.2, so I ran the following to install Telegraf on my machine:\nwget https://dl.influxdata.com/telegraf/releases/telegraf_1.14.2-1_armhf.deb\nsudo dpkg -i telegraf_1.14.2-1_armhf.deb\nrm telegraf_1.14.2-1_armhf.deb\n \n \nWe now have Telegraf installed on our machine, but the service won’t do us much good before we set up our configuration, located at /etc/telegraf/telegraf.conf. Telegraf operates by coordinating a bunch of “plugins”, which work to collect and write data to and from different sources. You can see the full list of plugins at Telegraf’s GitHub repo, and activate each by copying the configuration from the plugin’s readme into your /etc/telegraf/telegraf.conf file.\nI spent far too much time pouring over the various plugins and wound up with the following configuration file – you can use this to overwrite your default telegraph.conf file and start collecting metrics right away, or you can spend the time now to set up your instance to suit your own particular needs. Just make sure you edit your [[outputs.influxdb]] to include the following:\n[[outputs.influxdb]]\n   ## The full HTTP or UDP URL for your InfluxDB instance.\n   urls = [\"http://127.0.0.1:8086\"] # required\n \n \nMy full configuration looks like this:\n[agent]\n   # Batch size of values that Telegraf sends to output plugins.\n   metric_batch_size = 1000\n   # Default data collection interval for inputs.\n   interval = \"30s\"\n   # Added degree of randomness in the collection interval.\n   collection_jitter = \"5s\"\n   # Send output every 5 seconds\n   flush_interval = \"5s\"\n   # Buffer size for failed writes.\n   metric_buffer_limit = 10000\n   # Run in quiet mode, i.e don't display anything on the console.\n   quiet = true\n[[inputs.ping]] # # Ping given url(s) and return statistics\n## urls to ping\nurls = [\"www.github.com\",\"www.amazon.com\",\"1.1.1.1\",\"www.mm218.dev\"]\n## number of pings to send per collection (ping -c )\ncount = 3\n## interval, in s, at which to ping. 0 == default (ping -i )\nping_interval = 15.0\n## per-ping timeout, in s. 0 == no timeout (ping -W )\ntimeout = 10.0\n## interface to send ping from (ping -I )\ninterface = \"wlan0\"\n[[inputs.system]]\n[[inputs.influxdb]]\n  ## Works with InfluxDB debug endpoints out of the box,\n  ## but other services can use this format too.\n  ## See the influxdb plugin's README for more details.\n\n  ## Multiple URLs from which to read InfluxDB-formatted JSON\n  ## Default is \"http://localhost:8086/debug/vars\".\n  urls = [\n    \"http://localhost:8086/debug/vars\"\n  ]\n  ## http request & header timeout\n  timeout = \"5s\"\n[[inputs.disk]]\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n[[inputs.diskio]]\n[[inputs.internal]]\n  ## If true, collect telegraf memory stats.\n  collect_memstats = true\n[[inputs.mem]]\n[[inputs.processes]]\n# custom temperature script\n# https://github.com/mikemahoney218/pi-admin/blob/master/telegraf-scripts/systemp.sh\n[[inputs.exec]]\n  commands = [\"sh /tmp/telegraf-scripts/systemp.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n[[outputs.influxdb]]\n   ## The full HTTP or UDP URL for your InfluxDB instance.\n   urls = [\"http://127.0.0.1:8086\"] # required\n   ## The target database for metrics (telegraf will create it if not exists).\n   database = \"pi_logs\" # required\n   ## Name of existing retention policy to write to.  Empty string writes to\n   ## the default retention policy.\n   retention_policy = \"\"\n   ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\"\n   write_consistency = \"any\"\n   ## Write timeout (for the InfluxDB client), formatted as a string.\n   ## If not provided, will default to 5s. 0s means no timeout (not recommended).\n   timeout = \"10s\"\n \n \nIn putting all this together, I found out that the Telegraf plugin to measure system temperature – the thing that got me down this rabbit hole in the first place – doesn’t actually work on Raspberry Pi systems. As a workaround, I threw together a simple one-liner in Bash:\necho \"systemp temp=`cat /sys/class/thermal/thermal_zone0/temp`\"\n \n \nI saved that script off to /tmp/telegraf-scripts/systemp.sh, then added it to my telegraf.conf in the brick:\n[[inputs.exec]]\n  commands = [\"sh /tmp/telegraf-scripts/systemp.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n \n \nIf you’re not worried about measuring temperature, you don’t need (or want) to include that section in your telegraf.conf.\nIf you set up HTTP authentication for your Influx instance, you’re going to want to add username and password fields under the [[outputs.influxdb]]\nWith our configuration in place, all that’s left now is to start and enable the Telegraf service:\nsudo systemctl enable --now telegraf\n \n \nAs before, you should be able to see that the service is running without issue by running systemctl status telegraf.\nNow that your service is running, any changes that you make to your telegraf.config file will only take effect after the service restarts. You can always restart the service using sudo systemctl restart telegraf, but I personally kept forgetting to do so (and then was surprised when my metrics weren’t showing up in Influx). To deal with that, I wrote an extremely-micro service that restarts Telegraf for me.\n\nGrafana\nWe’re finally onto our last service, the G in the TIG stack, Grafana. A quick word of warning: don’t try to sudo apt install grafana. The main repository has an outdated version of Grafana, which will leave you stuck at a blank screen when you try to log on for the first time.\nInstead, we’ll install Grafana via dpkg, like we did with Telegraf. Check for the most current version at Grafana’s downloads page. At the time of writing, I was installing version 6.7.3, so my commands to install looked like this:\nwget https://dl.grafana.com/oss/release/grafana_6.7.3_armhf.deb\nsudo dpkg -i grafana_6.7.3_armhf.deb\nsudo systemctl enable --now grafana-server\nrm grafana_6.7.3_armhf.deb\n \n \nUnlike Influx and Telegraf, Grafana can be managed almost entirely from a UI. Boot up localhost:3000 on your Pi and log in using admin for both your username and password – you’ll be prompted to change it once you’re logged in for the first time.\nYou’ll then want to add your local Influx instance as a datasource for Grafana. Assuming you’ve followed along until now, the URL for your Influx instance is http://localhost:8086. You’ll also want to add whatever database Telegraf is writing to – in the sample configuration I posted, the database name is pi_logs, but you can find yours by looking for the database field under [[outputs.influxdb]]. If you added authentication to your Influx instance, you’ll also want to turn on basic auth and provide your database credentials.\n\nGet Graphing\nAnd with that, you should have everything you need to start monitoring your Pi – and, with a little elbow grease, anything your Pi can touch! While it certainly feels a little like overkill, I’ve now got state-of-the art tracking and system metrics for my Pi, letting me confirm beyond a shadow of a doubt that… my Pi is running too hot. With all the time I spent on this, maybe I should have just bought a fan.\nBut hey – would a fan look this good?\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020/04/corona-viz/",
    "title": "A minimalist visualization of Coronavirus rates",
    "description": "Made with Shiny and R",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2020-04-27",
    "categories": [
      "R"
    ],
    "contents": "\nI had been getting frustrated with not being able to quickly find coronavirus data for my area, and not being able to see recent trends without framing and interpretation. So I grabbed down the John Hopkins CSSE data and made a quick Shiny app to visualize case and death rates. I’m trying to not contribute to the constant noise surrounding the ongoing pandemic, but having a way to see these numbers without an overwhelming amount of surrounding editorialization has made me feel like I understand the world a bit better.\n\n\nknitr::include_graphics(\"covid.png\")\n\n\n\n\nThe app lives at this link. Thanks to R Studio, who are providing free hosting for coronavirus apps through the pandemic.\n\n\n\n\n",
    "preview": "posts/2020/04/corona-viz/covid.png",
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {},
    "preview_width": 942,
    "preview_height": 883
  },
  {
    "path": "posts/2020/04/making-excellent-viz/",
    "title": "Making Excellent Visualizations",
    "description": "Part 3 in the data visualization series",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2020-04-22",
    "categories": [
      "Data Visualization",
      "Tutorials"
    ],
    "contents": "\nAs we move into our final section, it’s time to dwell on our final mantra:\nInk is cheap. Electrons are even cheaper.\nThis is a fancy, dogmatic way to say: Make more than one chart. It’s rare that your first try is going to produce your best looking output. Play around with your data set, try out different visuals, and keep the concepts we’ve talked about in mind. Your graphs will be all the better for it. In this section, we’ll talk about solutions to some of the most common problems people have with making charts:\nDealing with big data sets\nThink back to the diamonds data set we used in the last section. It contains data on 54,000 individual diamonds, including the carat and sale price for each. If we wanted to compare those two continuous variables, we might think a scatter plot would be a good way to do so:\n\nUnfortunately, it seems like 54,000 points is a few too many for this plot to do us much good! This is a clear case of what’s called overplotting – we simply have too much data on a single graph.\nThere are three real solutions to this problem. First off, we could decide simply that we want to refactor our chart, and instead show how a metric – such as average sale price – changes at different carats, rather than how our data is distributed:\n\nThere are all sorts of ways we can do this sort of refactoring – if we wanted, we could get a very similar graph by binning our data and making a bar plot:\n\nEither way, though, we’re not truly showing the same thing as was in the original graph – we don’t have any indication of the actual distribution of our data set along these axes.\nThe second solution solves this problem much more effectively – make all your points semi-transparent:\n\nBy doing this, we’re now able to see areas where our data is much more densely distributed, something that was lost in the summary statistics – for instance, it appears that low-carat diamonds are much more tighly grouped than higher carat ones. We can also see some dark stripes at “round-number” values for carat – that indicates to me that our data has some integrity issues, if appraisers are more likely to give a stone a rounded number.\nThe challenge with this approach comes when we want to map a third variable – let’s use cut – in our graphic. We can try to change the aesthetics of our graph as usual:\n\nBut unfortunately the sheer number of points drowns out most of the variance in color and shape on the graphic. In this case, our best option may be to turn to option number three and facet our plots – that is, to split our one large plot into several small multiples:\n\nRemember: Ink is cheap. Electrons are even cheaper. Make more than one graph.\nBy splitting out our data into several smaller graphics, we’re much better able to see how the distribution shifts between our categories. In fact, we could use this technique to split our data even further, into a matrix of scatter plots showing how different groups are distributed:\n\nOne last, extremely helpful use of faceting is to split apart charts with multiple entangled lines: \nThese charts, commonly referred to as “spaghetti charts”, are usually much easier to use when split into small multiples:\n\nNow, one major drawback of facet charts is that they can make comparisons much harder – if, in our line chart, it’s more important to know that most clarities are similar in price at 2 carats than it is to know how the price for each clarity changes with carat, then the first chart is likely the more effective option. In those cases, however, it’s worth reassessing how many lines you actually need on your graph – if you only care about a few clarities, then only include those lines, and if you only care about a narrow band of prices or carats, window your data so that’s all you show. The goal is to make making comparisons easy, with the understanding that some comparisons are more important than others.\nDealing with chartjunk\nCast your mind back to the graphic I used as an example of an explanatory chart:\n\nYou might have noticed that this chart is differently styled from all the others in this course – it doesn’t have the grey background or grid lines or anything else.\nThink back to our second mantra: everything should be made as simple as possible, but no simpler. This chart reflects that goal. We’ve lost some of the distracting elements – the colored background and grid lines – and changed the other elements to make the overall graphic more effective. The objective is to have no extraneous element on the graph, so that it might be as expressive and effective as possible. This usually means using minimal colors, minimal text, and no grid lines. (After all, those lines are usually only useful in order to pick out a specific value – and if you’re expecting people to need specific values, you should give them a table!)\nThose extraneous elements are known as chartjunk. You see this a lot with graphs made in Excel – they’ll have dark backgrounds, dark lines, special shading effects or gradients that don’t encode information, or – worst of all – those “3D” bar/line/pie charts, because these things can be added with a single click. However, they tend to make your graphics less effective as they force the user to spend more time separating data from ornamentation. Everything should be made as simple as possible, but no simpler; every element of your graphic should increase expressiveness or effectiveness. In short: don’t try to pretty up your graph with non-useful elements.\nAnother common instance of chartjunk is animation in graphics. While animated graphics are exciting and trendy, they tend to reduce the effectiveness of your graphics because as humans, when something is moving we can’t focus on anything else. Check out these examples from the Harvard Vision Lab – they show just how hard it is to notice changes when animation is added. This isn’t to say you can never use animation – but its uses are best kept to times when your graphic looking cool is more important than it conveying information.\nCommon Mistakes\nAs we wind down this section, I want to touch on a few common mistakes that didn’t have a great home in any other section – mostly because we were too busy talking about good design principles.\nDual y axes\nChief amongst these mistakes are plots with two y axes, beloved by charlatans and financial advisors since days unwritten. Plots with two y axes are a great way to force a correlation that doesn’t really exist into existence on your chart, through manipulation of your units and axes. In almost every case, you should just make two graphs – ink is cheap. Electrons are even cheaper.\nFor an extremely entertaining read on this subject, check out this link. I’ve borrowed Kieran’s code for the below viz – look at how we can imply different things, just by changing how we scale our axes!\n\nOvercomplex visualizations\nAnother common issue in visualizations comes from the analyst getting a little too technical with their graphs. For instance, think back to our original diamonds scatter plot: \nLooking at this chart, we can see that carat and price have a positive correlation – as one increases, the other does as well. However, it’s not a linear relationship; instead, it appears that price increases faster as carat increases.\nThe more statistically-minded analyst might already be thinking that we could make this relationship linear by log-transforming the axes – and they’d be right! We can see a clear linear relationship when we make the transformation:\n\nUnfortunately, transforming your visualizations in this way can make your graphic hard to understand – in fact, only about 60% of professional scientists can even understand them. As such, transforming your axes like this tends to reduce the effectiveness of your graphic – this type of visualization should be reserved for exploratory graphics and modeling, instead.\nConclusion\nAnd that just about wraps up this introduction to the basic concepts of data visualizations. Hopefully you’ve picked up some concepts or vocabulary that can help you think about your own visualizations in your daily life. I wanted to close out here with a list of resources I’ve found helpful in making graphics – I’ll keep adding to this over time:\nWhen picking colors, I often find myself reaching for one of the following tools:\nColorBrewer provided most of the palettes for these graphics\nColorSupply makes picking custom colors easier\nViridis provides beautiful, colorblind-friendly palettes for use (though this resource is a little harder to understand)\n\nI used the following resources in putting this post together:\nHadley Wickham’s Stat 405 Course, particularly the lecture on effective visualizations (I’ve lifted “perceptual topology should match data toplogy”, “make important comparisons easy”, and “visualization is only one part of data analysis” directly from his slides)\nJeffrey Heer’s CSE 442 lecture on visualizations, particularly the definitions for expressiveness and effectiveness\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020/04/mechanics-of-viz/",
    "title": "Mechanics of Data Visualizations",
    "description": "Part 2 in the data visualization series",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2020-04-21",
    "categories": [
      "Data Visualization",
      "Tutorials"
    ],
    "contents": "\n(Note: this is part two of a three part series on data visualization, originally published on Towards Data Science in 2019.\nLet’s move from theoretical considerations of graphing to the actual building blocks you have at your disposal. As we do so, we’re also going to move on to mantra #2:\nEverything should be made as simple as possible – but no simpler.\nGraphs are inherently a 2D image of our data:\n\nThey have an x and a y scale, and - as in our scatter plot here - the position a point falls along each scale tells you how large its values are. But this setup only allows us to look at two variables in our data - and we’re frequently interested in seeing relationships between more than two variables.\nSo the question becomes: how can we visualize those extra variables? We can try adding another position scale:\n\n\nknitr::include_graphics(\"unnamed-chunk-2-1.png\")\n\n\n\n\nBut 3D images are hard to wrap your head around, complicated to produce, and not as effective in delivering your message. They do have their uses - particularly when you’re able to build real, physical 3D models, and not just make 3D shapes on 2D planes - but frequently aren’t worth the trouble.\nSo what tools do we have in our toolbox? The ones that are generally agreed upon (no, really - this is an area of active debate) fall into four categories:\nPosition (like we already have with X and Y)\nColor\nShape\nSize\nThese are the tools we can use to encode more information into our graphics. We’re going to call these aesthetics, but any number of other words could work - some people refer to them as scales, some as values. I call them aesthetics because that’s what my software of choice calls them - but the word itself comes from the fact that these are the things that change how your graph looks.\nFor what it’s worth, we’re using an EPA data set for this unit, representing fuel economy data from 1999 and 2008 for 38 popular models of car. “Hwy” is highway mileage, “displ” is engine displacement (so volume), and “cty” is city mileage. But frankly, our data set doesn’t matter right now - most of our discussion here is applicable to any data set you’ll pick up.\nWe’re going to go through each of these aesthetics, to talk about how you can encode more information in each of your graphics. Along the way, remember our mantras:\nA good graphic tells a story\nEverything should be made as simple as possible - but no simpler\nUse the right tool for the job\nInk is cheap. Electrons are even cheaper\nWe’ll talk about how these are applicable throughout this section.\nPosition\nLet’s start off discussing these aesthetics by finishing up talking about position. The distance of values along the x, y, or – in the case of our 3D graphic – z axes represents how large a particular variable is. People inherently understand that values further out on each axis are more extreme - for instance, imagine you came across the following graphic (made with simulated data):\n\nWhich values do you think are higher?\nMost people innately assume that the bottom-left hand corner represents a 0 on both axes, and that the further you get from that corner the higher the values are. This – relatively obvious – revelation hints at a much more important concept in data visualizations: perceptual topology should match data topology. Put another way, that means that values which feel larger in a graph should represent values that are larger in your data. As such, when working with position, higher values should be the ones further away from that lower left-hand corner – you should let your viewer’s subconscious assumptions do the heavy lifting for you.\nApplying this advice to categorical data can get a little tricky. Imagine that we’re looking at the average highway mileages for manufacturers of the cars in our data set:\n\nIn this case, the position along the x axis just represents a different car maker, in alphabetical order. But remember, position in a graph is an aesthetic that we can use to encode more information in our graphics. And we aren’t doing that here – for instance, we could show the same information without using x position at all:\n\nTry to compare Pontiac and Hyundai on the first graph, versus on this second one. If anything, removing our extraneous x aesthetic has made it easier to compare manufacturers. This is a big driver behind our second mantra – that everything should be made as simple as possible, but no simpler. Having extra aesthetics confuses a graph, making it harder to understand the story it’s trying to tell.\nHowever, when making a graphic, we should always be aiming to make important comparisons easy. As such, we should take advantage of our x aesthetic by arranging our manufacturers not alphabetically, but rather by their average highway mileage: \nBy reordering our graphic, we’re now able to better compare more similar manufacturers. It’s now dramatically faster to understand our visualization – closer comparisons are easier to make, so placing more similar values closer together makes them dramatically easier to grasp. Look at Pontiac vs Hyundai now, for instance. Generally speaking, don’t put things in alphabetical order - use the order you place things to encode additional information.\nAs a quick sidenote, I personally believe that, when working with categorical values along the X axis, you should reorder your values so the highest value comes first. For some reason, I just find having the tallest bar/highest point (or whatever is being used to show value) next to the Y axis line is much cleaner looking than the alternative:\n\nFor what it’s worth, I’m somewhat less dogmatic about this when the values are on the Y axis. I personally believe the highest value should always be at the top, as humans expect higher values to be further from that bottom left corner: \nHowever, I’m not as instantly repulsed by the opposite ordering as I am with the X axis, likely because the bottom bar/point being the furthest looks like a more natural shape, and is still along the X axis line: \nFor this, at least, your mileage may vary. Also, it’s worth pointing out how much cleaner the labels on this graph are when they’re on the Y axis - flipping your coordinate system, like we’ve done here, is a good way to display data when you’ve got an unwieldy number of categories.\nColor\nWhile we’ve done a good job covering the role position plays in communicating information, we’re still stuck on the same question we started off with: How can we show a third variable on the graph?\nOne of the most popular ways is to use colors to represent your third variable. It might be worth talking through how color can be used with a simulated data set. Take for example the following graph: \nAnd now let’s add color for our third variable: \nRemember: perceptual topology should match data topology. Which values are larger?\nMost people would say the darker ones. But is it always that simple? Let’s change our color scale to compare: \nSure, some of these colors are darker than others – but I wouldn’t say any of them tell me a value is particularly high or low.\nThat’s because humans don’t percieve hue – the actual shade of a color – as an ordered value. The color a point is doesn’t communicate that the point has a higher or lower value than any other point on the graph. Instead, hue works as an unordered value, which only tells us which points belong to which groupings. In order to tell how high or low a point’s value is, we instead have to use luminescence – or how bright or dark the individual point is.\nThere’s one other axis you can move colors along in order to encode value – how vibrant a color is, known as chroma:\n\nJust keep in mind that luminescence and chroma – how light a color is and how vibrant it is – are ordered values, while hue (or shade of color) is unordered This becomes relevant when dealing with categorical data. For instance, moving back to the scatter plot we started with:\n\nIf we wanted to encode a categorical variable in this – for instance, the class of vehicle – we could use hue to distinguish the different types of cars from one another:\n\nIn this case, using hue to distinguish our variables clearly makes more sense than using either chroma or luminesence:\n\nThis is a case of knowing what tool to use for the job - chroma and luminescence will clearly imply certain variables are closer together than is appropriate for categorical data, while hue won’t give your audience any helpful information about an ordered variable. Note, though, that I’d still discourage using the rainbow to distinguish categories in your graphics – the colors of the rainbow aren’t exactly unordered values (for instance, red and orange are much more similar colors than yellow and blue), and you’ll wind up implying connections between your categories that you might not want to suggest. Also, the rainbow is just really ugly:\n\nSpeaking of using the right tool for the job, one of the worst things people like to do in data visualizations is overuse color. Take for instance the following example:\n\nIn this graph, the variable “class” is being represented by both position along the x axis, and by color. By duplicating this effort, we’re making our graph harder to understand – encoding the information once is enough, and doing it any more times than that is a distraction. Remember the second mantra: Everything should be made as simple as possible – but no simpler. The best data visualization is one that includes all the elements needed to deliver the message, and no more.\nYou can feel free to use color in your graphics, so long as it adds more information to the plot - for instance, if it’s encoding a third variable:\n\nBut replicating as we did above is just adding more junk to your chart.\nThere’s one last way you can use color effectively in your plot, and that’s to highlight points with certain characteristics:\n\nDoing so allows the viewer to quickly pick out the most important sections of our graph, increasing its effectiveness. Note that I used shape instead of color to separate the class of vehicles, by the way – combining point highlighting and using color to distinguish categorical variables can work, but can also get somewhat chaotic:\n\nThere’s one other reason color is a tricky aesthetic to get right in your graphics: about 5% of the population (10% of men, 1% of women) can’t see colors at all. That means you should be careful when using it in your visualizations – use colorblind-safe color palettes (google “ColorBrewer” or “viridis” for more on these), and pair it with another aesthetic whenever possible.\nShape\nThe easiest aesthetic to pair color with is the next most frequently used – shape. This one is much more intuitive than color – to demonstrate, let’s go back to our scatter plot:\n\nWe can now change the shape of each point based on what class of vehicle it represents: \nImagine we were doing the same exercise as we did with color earlier – which values are larger?\nI’ve spoiled the answer already by telling you what the shapes represent – none of them are inherently larger than the others. Shape, like hue, is an unordered value.\nThe same basic concepts apply when we change the shape of lines, not just points. For instance, if we plot separate trendlines for front-wheel, rear-wheel, and four-wheel drive cars, we can use linetype to represent each type of vehicle:\n\nBut even here, no one linetype implies a higher or lower value than the others.\nThere are two caveats to be made to this rule, however. For instance, if we go back to our original scatter plot and change which shapes we’re using:\n\nThis graph seems to imply more connection between the first three classes of car (which are all different types of diamonds) and the next three classes (which are all types of triangle), while singling out SUVs. In this way, we’re able to use shape to imply connection between our groupings - more similar shapes, which differ only in angle or texture, imply a closer relationship to one another than to other types of shape. This can be a blessing as well as a curse - if you pick, for example, a square and a diamond to represent two unrelated groupings, your audience might accidentally read more into the relationship than you had meant to imply.\nIt’s also worth noting that different shapes can pretty quickly clutter up a graph. As a general rule of thumb, using more than 3-4 shapes on a graph is a bad idea, and more than 6 means you need to do some thinking about what you actually want people to take away.\nSize\nOur last aesthetic is that of size. Going back to our original scatter plot, we could imagine using size like this:\n\nSize is an inherently ordered value - large size points imply larger values. Specifically, humans perceive larger areas as corresponding to larger values - the points which are three times larger in the above graph are about three times larger in value, as well.\nThis becomes tricky when size is used incorrectly, either by mistake or to distort the data. Sometimes an analyst maps radius to the variable, rather than area of the point, resulting in graphs as the below:\n\nIn this example, the points representing a cty value of 10 don’t look anything close to 1/3 as large as the points representing 30. This makes the increase seem much steeper upon looking at this chart – so be careful when working with size as an aesthetic that your software is using the area of points, not radius!\nIt’s also worth noting that unlike color – which can be used to distinguish groupings, as well as represent an ordered value – it’s generally a bad idea to use size for a categorical variable. For instance, if we mapped point size to class of vehicle:\n\nWe seem to be implying relationships here that don’t actually exist, like a minivan and midsize vehicle being basically the same. As a result, it’s best to only use size for continuous (or numeric) data.\nA Tangent\nNow that we’ve gone over these four aesthetics, I want to go on a quick tangent. When it comes to how quickly and easily humans perceive each of these aesthetics, research has settled on the following order:\nPosition\nSize\nColor (especially chroma and luminescence)\nShape\nAnd as we’ve discussed repeatedly, the best data visualization is one that includes exactly as many elements as it takes to deliver a message, and no more. Everything should be made as simple as possible, but no simpler.\nHowever, we live in a world of humans, where the scientifically most effective method is not always the most popular one. And since color is inherently more exciting than size as an aesthetic, the practitioner often finds themselves using colors to denote values where size would have sufficed. And since we know that color should usually be used alongside shape in order to be more inclusive in our visualizations, size often winds up being the last aesthetic used in a chart. This is fine - sometimes we have to optimize for other things than “how quickly can someone understand my chart”, such as “how attractive does my chart look” or “what does my boss want from me”. But it’s worth noting, in case you see contradictory advice in the future - the disagreement comes from if your source is teaching the most scientifically sound theory, or the most applicable practice.\nSummary\nWe started off this section with our second mantra: that everything should be made as simple as possible, but no simpler. The first half of that cautions us against overusing aesthetics and against adding too much to a graphic, lest we erode its efficency in conveying information:\n\nThe second half cautions us against not using all the aesthetics it takes to tell our story, in case we don’t produce the most expressive graphic possible: \nInstead, we should use exactly as many aesthetics as it takes to tell our story, carefully choosing each to encode the most information possible into our graphics: \nAs for the specific takeaways from this section, I can think of the following:\nMatch perceptual and data topology – if a color or position feels like a higher value, use it to represent data that is a higher value\nMake important comparisons easy – place them near each other, call attention to them\nUse aesthetics to encode more information into your graphics\nUse exactly as many aesthetics as you need – no more, no less.\n\nDon’t place things in alphabetical order\nDon’t use the rainbow for a color scheme\nUse ordered aesthetics (like position, chroma, luminescence, and size) to show ordered values (like numeric data)\nUse unordered aesthetics (like hue or shape) to show unordered values\nLet’s transition away from aesthetics, and towards our third mantra:\nUse the right tool for the job.\nThink back to our first chart:\n\nAs you already know, this is a scatter plot - also known as a point graph. Now say we added a line of best fit to it:\n\nThis didn’t stop being a scatter plot once we drew a line on it – but the term scatter plot no longer really encompasses everything that’s going on here. It’s also obviously not a line chart, as even though there’s a line on it, it also has points.\nRather than quibble about what type of chart this is, it’s more helpful to describe what tools we’ve used to depict our data. We refer to these as geoms, short for geometries – because when you get really deep into things, these are geometric representations of how your data set is distributed along the x and y axes of your graph. I don’t want to get too far down that road – I just want to explain the vocabulary so that we aren’t talking about what type of chart that is, but rather what geoms it uses. Framing things that way makes it easier to understand how things can be combined and reformatted, rather than assuming each type of chart can only do one thing.\nTwo continuous variables\nThis chart uses two geoms that are really good for graphs that have a continuous y and a continuous x - points and lines. This is what people refer to most of the time when they say a line graph - a single smooth trendline that shows a pattern in the data. However, a line graph can also mean a chart where each point is connected in turn:\n\nIt’s important to be clear about which type of chart you’re expected to produce! I always refer to the prior as a trendline, for clarity.\nThese types of charts have enormous value for quick exploratory graphics, showing how various combinations of variables interact with one another. For instance, many analysts start familiarizing themselves with new data sets using correlation matrices (also known as scatter plot matrices), which create a grid of scatter plots representing each variable:\n\nIn this format, understanding interactions between your data is quick and easy, with certain variable interactions obviously jumping out as promising avenues for further exploration.\nTo back up just a little, there’s one major failing of scatter plots that I want to highlight before moving on. If you happen to have more than one point with the same x and y values, a scatter plot will just draw each point over the previous, making it seem like you have less data than you actually do. Adding a little bit of random noise - for instance, using RAND() in Excel - to your values can help show the actual densities of your data, especially when you’re dealing with numbers that haven’t been measured as precisely as they could a have been. \nOne last chart that does well with two continuous variables is the area chart, which resembles a line chart but fills in the area beneath the line: \nArea plots make sense when 0 is a relevant number to your data set – that is, a 0 value wouldn’t be particularly unexpected. They’re also frequently used when you have multiple groupings and care about their total sum:\n\n(This new data set is the “diamonds” data set, representing 54,000 diamonds sizes, qualities, cut, and sale prices. We’ll be going back and forth using it and the EPA data set from now on.)\nNow one drawback of stacked area charts is that it can be very hard to estimate how any individual grouping shifts along the x axis, due to the cumulative effects of all the groups underneath them. For instance, there are actually fewer “fair” diamonds at 0.25 carats than at 1.0 – but because “ideal” and “premium” spike so much, your audience might draw the wrong conclusions. In situations where the total matters more than the groupings, this is alright – but otherwise, it’s worth looking at other types of charts as a result.\nOne continuous variable\nIf instead you’re looking to see how a single continuous variable is distributed throughout your data set, one of the best tools at your disposal is the histogram. A histogram shows you how many observations in your data set fall into a certain range of a continuous variable, and plot that count as a bar plot:\n\nOne important flag to raise with histograms is that you need to pay attention to how your data is being binned. If you haven’t picked the right width for your bins, you might risk missing peaks and valleys in your data set, and might misunderstand how your data is distributed – for instance, look what shifts if we graph 500 bins, instead of the 30 we used above: \nAn alternative to the histogram is the frequency plot, which uses a line chart in the place of bars to represent the frequency of a value in your data set: \nAgain, however, you have to pay attention to how wide your data bins are with these charts – you might accidentally smooth over major patterns in your data if you aren’t careful! \nOne large advantage of the frequency chart over the histogram is how it deals with multiple groupings – if your groupings trade dominance at different levels of your variable, the frequency graph will make it much more obvious how they shift than a histogram will.\n(Note that I’ve done something weird to the data in order to show how the distributions change below.) \nOne categorical variable, one continuous\nIf you want to compare a categorical and continuous variable, you’re usually stuck with some form of bar chart:\n\nThe bar chart is possibly the least exciting type of graph in existence, mostly because of how prevalent it is – but that’s because it’s really good at what it does. Bar charts are one of the most easily interpreted and effective types of visualizations, no matter how exciting they are.\nHowever, some people are really intent on ruining that. Take, for instance, the stacked bar chart, often used to add a third variable to the mix:\n\nCompare Fair/G to Premium/G. It’s next to impossible to accurately compare the boxes – they don’t share a top or a bottom line, so you can’t really make a comparison. In these situations, it’s a better idea to use a dodged bar chart instead:\n\nDodged bar charts are usually a better choice for comparing the actual numbers of different groupings. However, this chart does a good job showing one of the limitations dodged bar charts come up against – once you get past 4 or 5 groupings, making comparisons is tricky. In these cases, you’re probably trying to apply the wrong chart for the job, and should consider either breaking your chart up into smaller ones – remember, ink is cheap, and electrons or cheaper – or replacing your bars with a few lines.\nThe one place where stacked bar charts are appropriate, however, is when you’re comparing the relative proportions of two different groups in each bar. For instance, take the following graph:\n\nIn this case, making comparisons across groups is trivial, made simple by the fact that the groupings all share a common line - at 100% for group 1, and at 0% for group 2. This point of reference solves the issue we had with more than two groupings – though note we’d still prefer a dodged bar chart if the bars didn’t always sum to the same amount.\nA Quick Tangent\nThis is usually where most people will go on a super long rant about pie charts and how bad they are. They’re wrong, but in an understandable way.\nPeople love to hate on pie charts, because they’re almost universally a bad chart. However, if it’s important for your viewer to be able to quickly figure out what proportion two or more groupings make up of the whole, a pie chart is actually the fastest and most effective way to get the point across. For instance, compare the following pie and bar charts, made with the same data set: \n\nIt’s a lot easier to tell that, say, A is smaller than C through F in the pie chart than the bar plot, since humans are better at summing angles than areas. In these instances, feel free to use a pie chart – and to tell anyone giving you flack that I said it was OK.\nTwo categorical variables\nOur last combination is when you’re looking to have a categorical variable on both the x and y axis. These are trickier plots to think about, as we no longer encode value in position based on how far away a point is from the lower left hand corner, but rather have to get creative in effectively using position to encode a value. Remember that a geom is a geometric representation of how your data set is distributed along the x and y axes of your graph. When both of your axes are categorical, you have to get creative to show that distribution.\nOne method is to use density, as we would in a scatter plot, to show how many datapoints you have falling into each combination of categories graphed. You can do this by making a “point cloud” chart, where more dense clouds represent more common combinations: \nEven without a single number on this chart, its message is clear - we can tell how our diamonds are distributed with a single glance. A similar way to do this is to use a heatmap, where differently colored cells represent a range of values:\n\nI personally think heatmaps are less effective – partially because by using the color aesthetic to encode this value, you can’t use it for anything else – but they’re often easier to make with the resources at hand.\n\n\n\n\n",
    "preview": "posts/2020/04/mechanics-of-viz/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {},
    "preview_width": 672,
    "preview_height": 480
  },
  {
    "path": "posts/2020/04/theory-of-data-viz/",
    "title": "Theory of Data Visualizations",
    "description": "Part 1 in the data visualization series",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2020-04-20",
    "categories": [
      "Data Visualization",
      "Tutorials"
    ],
    "contents": "\n(Note: this is part one of a three part series on data visualization, originally published on Towards Data Science in 2019.\nData visualization – our working definition will be “the graphical display of data” – is one of those things like driving, cooking, or being funny: everyone thinks they’re really great at it, because they’ve been doing it for a while, and yet many – if not most – people don’t even know where they could start learning how much better they could be doing things. For something so essential to so many people’s daily work, data visualization is so rarely directly taught, and is usually assumed to be something people will pick up with time.\nHowever, that isn’t the best approach. Data visualization is a skill like any other, and even experienced practitioners could benefit from honing their skills in the subject. Hence, this series.\nThis series doesn’t set out to teach you how to make a specific graphic in a specific software. I don’t know what softwares might be applicable to your needs in the future, or what visualizations you’ll need to formulate when, and quite frankly Google exists – so this isn’t a cookbook with step-by-step instructions. The goal here is not to provide you with recipes for the future, but rather to teach you what flour is – to introduce you to the basic concepts and building blocks of effective data visualizations.\nThe mantras\nAs much as possible, I’ve collapsed those concepts into four mantras we’ll return to throughout this course. The mantras are:\nA good graphic tells a story.\nEverything should be made as simple as possible, but no simpler.\nUse the right tool for the job.\nInk is cheap. Electrons are even cheaper.\nEach mantra serves as the theme for a section, and will also be interwoven throughout. The theme of this section is, easily enough:\nA good graphic tells a story\nWhen making a graphic, it is important to understand what the graphic is for. After all, you usually won’t make a chart that is an exact depiction of your data – modern data sets tend to be too big (in terms of number of observations) and wide (in terms of number of variables) to depict every datapoint on a single graph. Instead, the analyst consciously chooses what elements to include in a visualization in order to identify patterns and trends in the data in the most effective manner possible. In order to make those decisions, it helps a little to think both about why and how graphics are made.\nWhy do we tell a story?\nAs far as the why question goes, the answer usually comes down to one of two larger categories:\nTo help identify patterns in a data set, or\nTo explain those patterns to a wider audience\nThese are the rationales behind creating what are known as, respectively, exploratory and explanatory graphics. Exploratory graphics are often very simple pictures of your data, built to identify patterns in your data that you might not know exist yet. Take for example a simple graphic, showing tree circumference as a function of age:\n\nThis visualization isn’t anything too complex – two variables, thirty-five observations, not much text – but it already shows us a trend that exists in the data. We could use this information, if we were so inspired, to start investigating the whys of why tree growth changes with age, now that we’re broadly aware of how it changes.\nExplanatory graphs, meanwhile, are all about the whys. Where an exploratory graphic focuses on identifying patterns in the first place, an explanatory graphic aims to explain why they happen and – in the best examples – what exactly the reader is to do about them. Explanatory graphics can exist on their own or in the context of a larger report, but their goals are the same: to provide evidence about why a pattern exists and provide a call to action. For instance, we can reimagine the same tree graph with a few edits in order to explain what patterns we’re seeing:\n\n\nknitr::include_graphics(\"theory2.png\")\n\n\n\n\nI want to specifically call out the title here: “Orange tree growth tapers by year 4.” A good graphic tells a story, remember. As such, whatever title you give your graph should reflect the point of that story – titles such as “Tree diameter (cm) versus age (days)” and so on add nothing that the user can’t get from the graphic itself. Instead, use your title to advance your message whenever it makes sense – otherwise, if it doesn’t add any new information, you’re better off erasing it altogether.\nThe important takeaway here is not that explanatory graphics are necessarily more polished than exploratory ones, or that exploratory graphics are only for the analyst – periodic reporting, for instance, will often use highly polished exploratory graphics to identify existing trends, hoping to spur more intensive analysis that will identify the whys. Instead, the message is that knowing the end purpose of your graph – whether it should help identify patterns in the first place or explain how they got there – can help you decide what elements need to be included to tell the story your graphic is designed to address.\nHow do we tell a story?\nThe other important consideration when thinking about graph design is the actual how you’ll tell your story, including what design elements you’ll use and what data you’ll display. My preferred paradigm when deciding between the possible “hows” is to weigh the expressiveness and effectiveness of the resulting graphic – as defined by Jeffrey Heer at the University of Washington, that means:\nExpressiveness: A set of facts is expressible in a visual language if the sentences (i.e. the visualizations) in the language express all the facts in the set of data, and only the facts in the data.\nEffectiveness: A visualization is more effective than another visualization if the information conveyed by one visualization is more readily perceived than the information in the other visualization.\nOr, to simplify:\nTell the truth and nothing but the truth (don’t lie, and don’t lie by omission)\nUse encodings that people decode better (where better = faster and/or more accurate)\nKeep this concept in the back of your mind as we move into the mechanics of data visualization in our next post – it should be your main consideration while deciding which elements you use! We’ll keep returning to these ideas of explanatory and exploratory, as well as expressiveness and effectiveness, throughout the rest of this article.\n\n\n\n\n",
    "preview": "posts/2020/04/theory-of-data-viz/theory2.png",
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {},
    "preview_width": 672,
    "preview_height": 480
  },
  {
    "path": "posts/2020/03/",
    "title": "Announcing {spacey}, now on CRAN!",
    "description": "USGS data access and rayshader maps, done cheap.",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2020-03-24",
    "categories": [
      "R",
      "R Packages",
      "maps",
      "spacey",
      "geospatial data"
    ],
    "contents": "\nI’ve launched a new package to CRAN! spacey helps you pull elevation and image overlay data from the USGS and ESRI, then helps you turn them into beautiful maps via the fantastic rayshader package.\nThe package has a documentation website built with pkgdown – check it out for more information!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020/01/",
    "title": "Announcing {heddlr}, now on CRAN!",
    "description": "Write less boilerplate, get more done.",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2020-01-23",
    "categories": [
      "R",
      "R Packages",
      "R Markdown"
    ],
    "contents": "\nMy first package just got published to CRAN today! heddlr is a set of tools that make it easier to write modular R Markdown documents by decomposing them into a set of patterns which can be repeated and combined based on your input data, letting you dynamically add and remove sections based on your data. I started this package to solve an issue I found myself running into when building flexdashboards, and have since found out that there’s all sorts of cool tricks you can do by applying this type of functional programming mindset to R Markdown documents.\nYou can find out more on heddlr’s documentation website, proudly made in R via pkgdown. This first version on CRAN is 0.5.0, with 0.1 -> 0.4 previously released on GitHub.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019/03/",
    "title": "Thesis Now Available in ESF Digital Commons",
    "description": "It's a real humdinger.",
    "author": [
      {
        "name": "Mike Mahoney",
        "url": {}
      }
    ],
    "date": "2019-03-27",
    "categories": [
      "Publications",
      "Beaver"
    ],
    "contents": "\nMy thesis is now available on the ESF Digital Commons! I’m extremely grateful to Drs. John Drake and Bill Shields for their help in the revision and submission process, and of course to Dr. John Stella for the extensive support he provided throughout the project, from conceptualization to publication.\nIn the thesis, we look at the impacts beavers have on the forest community around them as they remove trees for food and building dams. While people had looked at these impacts in other parts of beaver’s range, the Adirondacks are a strange enough ecosystem - being largely protected from anthropogenic disturbances, most of the forest landscape exhibits only one or two age classes - that we weren’t sure how applicable conclusions from these regions would be. What we found was that while the broad conclusions of these studies held true - beavers still operate as central place foragers and create large disturbances in the landscape - the lack of early-successional species throughout the Adirondacks seriously shifted which stems were harvested preferrentially. We also found a lot of variance in the patterns of how individual species were utilized - for instance, beaver harvested almost any size speckled alder they could find, so long as it was close to their dam, but would harvest red maple at any distance, so long as the stem was small.\n\nWe’re currently working on a journal article version of the thesis, using an expanded dataset and focusing more closely on the patterns in forage selectivity we found, and how they differ from other regions. That should hopefully be in the review process within the next few weeks.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-16T20:06:20-04:00",
    "input_file": {}
  }
]
